"""
ê³ ê¸‰ ê¸°ëŠ¥ ê°œì„  ìŠ¤í¬ë¦½íŠ¸

ê°œì„  ì˜ì—­:
1. Horizon 3ê°œì›” ì„±ëŠ¥ ê°œì„  (RÂ²=77~87% â†’ 90%+ ëª©í‘œ)
2. XAI (SHAP ê¸°ë°˜ Feature ê¸°ì—¬ë„ ì‹œê°í™”)
3. Confidence Interval (90% CI ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±)
4. ì„¸ê·¸ë¨¼íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ (K-Means/DBSCAN)
5. ì•¡ì…˜ ROI ì¶”ì • (ì˜ˆìƒ íš¨ê³¼ ê¸ˆì•¡)
6. ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ (Best/Base/Worst Case)
7. ì™¸ë¶€ ë°ì´í„° í™•ì¥ (ESI, KASI ë“± ì¶”ê°€ í†µí•©)
8. Unit Test ì»¤ë²„ë¦¬ì§€ (pytest ê¸°ë°˜)

Author: iM ONEderful Team
Date: 2026-01-14
"""

import argparse
import warnings
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass
import json

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score, r2_score
import pickle

warnings.filterwarnings('ignore')

# ============================================================================
# 1. Horizon 3ê°œì›” ì„±ëŠ¥ ê°œì„ 
# ============================================================================

class HorizonOptimizer:
    """Horizon 3ê°œì›” ì˜ˆì¸¡ ì •í™•ë„ ê°œì„ """
    
    def __init__(self, panel: pd.DataFrame, cfg=None):
        self.panel = panel
        self.cfg = cfg
        
    def create_enhanced_features(self) -> pd.DataFrame:
        """3ê°œì›” ì˜ˆì¸¡ì„ ìœ„í•œ ê°•í™”ëœ í”¼ì²˜ ìƒì„± (ìµœì í™” ë²„ì „)"""
        df = self.panel.copy()
        df = df.sort_values(['segment_id', 'month'])
        
        # ì¥ê¸° íŠ¸ë Œë“œ í”¼ì²˜ (3, 6ê°œì›”) - ë²¡í„°í™” ì—°ì‚°
        kpi_cols = ['log1p_ì˜ˆê¸ˆì´ì”ì•¡', 'log1p_ëŒ€ì¶œì´ì”ì•¡', 'log1p_ì¹´ë“œì´ì‚¬ìš©', 
                    'log1p_ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡', 'slog1p_ìˆœìœ ì…']
        
        existing_cols = [c for c in kpi_cols if c in df.columns]
        
        print(f"    í”¼ì²˜ ìƒì„± ëŒ€ìƒ: {len(existing_cols)} KPIs")
        
        for col in existing_cols:
            # ì´ë™í‰ê·  (vectorized)
            df[f"{col}_ma3"] = df.groupby('segment_id')[col].transform(
                lambda x: x.rolling(3, min_periods=1).mean()
            )
            
            # íŠ¸ë Œë“œ ê¸°ìš¸ê¸° (í˜„ì¬ vs 3ê°œì›” ì „)
            df[f"{col}_lag3"] = df.groupby('segment_id')[col].shift(3)
            df[f"{col}_trend_slope"] = (df[col] - df[f"{col}_lag3"]) / (df[f"{col}_lag3"].abs() + 1e-9)
            df = df.drop(columns=[f"{col}_lag3"])
        
        print(f"    ìƒì„± ì™„ë£Œ: {len(df.columns)} total columns")
        
        return df
    
    def get_horizon3_optimized_params(self) -> Dict:
        """Horizon 3 ìµœì í™”ëœ LightGBM íŒŒë¼ë¯¸í„°"""
        return {
            'n_estimators': 500,
            'learning_rate': 0.03,  # ëŠë¦° í•™ìŠµë¥ 
            'num_leaves': 63,
            'max_depth': 8,
            'min_child_samples': 50,
            'subsample': 0.7,
            'colsample_bytree': 0.7,
            'reg_alpha': 0.5,
            'reg_lambda': 0.5,
            'random_state': 42,
        }


# ============================================================================
# 2. XAI - SHAP ê¸°ë°˜ Feature ê¸°ì—¬ë„
# ============================================================================

@dataclass
class FeatureContribution:
    """Feature ê¸°ì—¬ë„ ë°ì´í„° í´ë˜ìŠ¤"""
    feature: str
    shap_value: float
    feature_value: float
    contribution_pct: float
    direction: str  # 'positive' or 'negative'


class XAIExplainer:
    """SHAP ê¸°ë°˜ ì„¤ëª…ê°€ëŠ¥ AI"""
    
    def __init__(self, model, feature_names: List[str]):
        self.model = model
        self.feature_names = feature_names
        self.shap_values = None
        
    def compute_shap_values(self, X: pd.DataFrame, sample_size: int = 1000) -> np.ndarray:
        """SHAP ê°’ ê³„ì‚° (TreeExplainer ê¸°ë°˜)"""
        try:
            import shap
            
            # ìƒ˜í”Œë§ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
            if len(X) > sample_size:
                X_sample = X.sample(n=sample_size, random_state=42)
            else:
                X_sample = X
            
            # TreeExplainer (LightGBM ìµœì í™”)
            explainer = shap.TreeExplainer(self.model)
            self.shap_values = explainer.shap_values(X_sample)
            
            return self.shap_values
        except ImportError:
            print("âš ï¸ SHAP ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜. Feature Importanceë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            return self._compute_fallback_importance(X)
    
    def _compute_fallback_importance(self, X: pd.DataFrame) -> np.ndarray:
        """SHAP ë¯¸ì„¤ì¹˜ ì‹œ Fallback: LightGBM Feature Importance ê¸°ë°˜"""
        if hasattr(self.model, 'feature_importances_'):
            imp = self.model.feature_importances_
            # ê° ìƒ˜í”Œì— ëŒ€í•´ ë™ì¼í•œ ì¤‘ìš”ë„ ë°°ë¶„ (ê·¼ì‚¬)
            return np.tile(imp, (len(X), 1))
        return np.zeros((len(X), len(self.feature_names)))
    
    def get_top_features(self, X: pd.DataFrame, top_k: int = 10) -> pd.DataFrame:
        """ìƒìœ„ Kê°œ ì¤‘ìš” í”¼ì²˜"""
        if self.shap_values is None:
            self.compute_shap_values(X)
        
        # í‰ê·  ì ˆëŒ€ê°’ ê¸°ì¤€
        mean_abs_shap = np.abs(self.shap_values).mean(axis=0)
        
        df = pd.DataFrame({
            'feature': self.feature_names,
            'mean_abs_shap': mean_abs_shap,
            'mean_shap': self.shap_values.mean(axis=0),
        })
        df['direction'] = df['mean_shap'].apply(lambda x: 'positive' if x > 0 else 'negative')
        df['contribution_pct'] = df['mean_abs_shap'] / df['mean_abs_shap'].sum() * 100
        
        return df.sort_values('mean_abs_shap', ascending=False).head(top_k)
    
    def explain_segment(self, X_segment: pd.Series, top_k: int = 5) -> List[FeatureContribution]:
        """íŠ¹ì • ì„¸ê·¸ë¨¼íŠ¸ ì˜ˆì¸¡ ì„¤ëª…"""
        if self.shap_values is None:
            print("âš ï¸ SHAP ê°’ì´ ê³„ì‚°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return []
        
        # ë‹¨ì¼ ìƒ˜í”Œ SHAP (ê·¼ì‚¬)
        contributions = []
        for i, feat in enumerate(self.feature_names):
            if i < len(X_segment):
                shap_val = self.shap_values[0][i] if len(self.shap_values) > 0 else 0
                contributions.append(FeatureContribution(
                    feature=feat,
                    shap_value=shap_val,
                    feature_value=X_segment.iloc[i] if i < len(X_segment) else 0,
                    contribution_pct=abs(shap_val),
                    direction='positive' if shap_val > 0 else 'negative'
                ))
        
        contributions.sort(key=lambda x: abs(x.shap_value), reverse=True)
        return contributions[:top_k]


# ============================================================================
# 3. Confidence Interval (ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±)
# ============================================================================

class ConfidenceIntervalEstimator:
    """ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„± êµ¬ê°„ ì¶”ì •"""
    
    def __init__(self, model, residual_std: float = 0.0):
        self.model = model
        self.residual_std = residual_std
        self.quantile_models = {}
        
    def fit_quantile_regression(self, X_train: pd.DataFrame, y_train: np.ndarray,
                                 quantiles: List[float] = [0.05, 0.5, 0.95]):
        """Quantile Regression ê¸°ë°˜ CI"""
        try:
            import lightgbm as lgb
            
            for q in quantiles:
                model = lgb.LGBMRegressor(
                    objective='quantile',
                    alpha=q,
                    n_estimators=100,
                    learning_rate=0.1,
                    num_leaves=31,
                    random_state=42,
                    verbosity=-1,
                    n_jobs=-1
                )
                model.fit(X_train, y_train)
                self.quantile_models[q] = model
                
        except Exception as e:
            print(f"âš ï¸ Quantile Regression ì‹¤íŒ¨: {e}")
    
    def predict_with_ci(self, X: pd.DataFrame, ci_level: float = 0.90) -> pd.DataFrame:
        """CI í¬í•¨ ì˜ˆì¸¡"""
        alpha = (1 - ci_level) / 2
        
        if self.quantile_models:
            # Quantile Regression ê¸°ë°˜
            lower = self.quantile_models.get(alpha, None)
            upper = self.quantile_models.get(1 - alpha, None)
            point = self.quantile_models.get(0.5, None)
            
            results = pd.DataFrame({
                'predicted': point.predict(X) if point else self.model.predict(X),
                'ci_lower': lower.predict(X) if lower else np.nan,
                'ci_upper': upper.predict(X) if upper else np.nan,
            })
        else:
            # ì”ì°¨ ê¸°ë°˜ ì •ê·œë¶„í¬ ê°€ì •
            from scipy import stats
            z = stats.norm.ppf(1 - alpha)
            
            pred = self.model.predict(X)
            results = pd.DataFrame({
                'predicted': pred,
                'ci_lower': pred - z * self.residual_std,
                'ci_upper': pred + z * self.residual_std,
            })
        
        results['ci_width'] = results['ci_upper'] - results['ci_lower']
        results['ci_level'] = ci_level
        
        return results


# ============================================================================
# 4. ì„¸ê·¸ë¨¼íŠ¸ í´ëŸ¬ìŠ¤í„°ë§
# ============================================================================

@dataclass
class ClusterResult:
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼"""
    cluster_id: int
    size: int
    centroid: np.ndarray
    silhouette: float
    top_segments: List[str]
    characteristics: Dict[str, float]


class SegmentClusterer:
    """ì„¸ê·¸ë¨¼íŠ¸ í´ëŸ¬ìŠ¤í„°ë§ (K-Means + DBSCAN)"""
    
    def __init__(self, n_clusters: int = 5):
        self.n_clusters = n_clusters
        self.kmeans = None
        self.dbscan = None
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=10)
        
    def prepare_segment_features(self, panel: pd.DataFrame) -> pd.DataFrame:
        """í´ëŸ¬ìŠ¤í„°ë§ìš© ì„¸ê·¸ë¨¼íŠ¸ í”¼ì²˜ ì§‘ê³„"""
        # ì„¸ê·¸ë¨¼íŠ¸ë³„ ìµœì‹  3ê°œì›” ì§‘ê³„
        latest_month = panel['month'].max()
        recent = panel[panel['month'] >= latest_month - pd.DateOffset(months=2)]
        
        agg_features = {}
        kpi_cols = ['log1p_ì˜ˆê¸ˆì´ì”ì•¡', 'log1p_ëŒ€ì¶œì´ì”ì•¡', 'log1p_ì¹´ë“œì´ì‚¬ìš©', 
                    'log1p_ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡', 'slog1p_ìˆœìœ ì…']
        
        for col in kpi_cols:
            if col in recent.columns:
                agg_features[f"{col}_mean"] = (col, 'mean')
                agg_features[f"{col}_std"] = (col, 'std')
                agg_features[f"{col}_trend"] = (col, lambda x: x.iloc[-1] - x.iloc[0] if len(x) > 1 else 0)
        
        segment_features = recent.groupby('segment_id').agg(**agg_features).reset_index()
        segment_features = segment_features.fillna(0)
        
        return segment_features
    
    def fit_kmeans(self, X: np.ndarray) -> np.ndarray:
        """K-Means í´ëŸ¬ìŠ¤í„°ë§"""
        # ìŠ¤ì¼€ì¼ë§ + PCA
        X_scaled = self.scaler.fit_transform(X)
        
        if X.shape[1] > 10:
            X_reduced = self.pca.fit_transform(X_scaled)
        else:
            X_reduced = X_scaled
        
        # ìµœì  K íƒìƒ‰ (Silhouette)
        best_k = self.n_clusters
        best_score = -1
        
        for k in range(2, min(10, len(X) // 10)):
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_reduced)
            score = silhouette_score(X_reduced, labels)
            
            if score > best_score:
                best_score = score
                best_k = k
        
        # ìµœì  Kë¡œ í•™ìŠµ
        self.kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
        labels = self.kmeans.fit_predict(X_reduced)
        
        print(f"  âœ“ K-Means: {best_k} clusters (Silhouette: {best_score:.4f})")
        
        return labels
    
    def fit_dbscan(self, X: np.ndarray, eps: float = 0.5, min_samples: int = 5) -> np.ndarray:
        """DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (ì´ìƒì¹˜ íƒì§€)"""
        X_scaled = self.scaler.transform(X) if hasattr(self.scaler, 'mean_') else self.scaler.fit_transform(X)
        
        self.dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        labels = self.dbscan.fit_predict(X_scaled)
        
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        n_outliers = (labels == -1).sum()
        
        print(f"  âœ“ DBSCAN: {n_clusters} clusters, {n_outliers} outliers")
        
        return labels
    
    def get_cluster_summary(self, segment_features: pd.DataFrame, 
                            labels: np.ndarray) -> List[ClusterResult]:
        """í´ëŸ¬ìŠ¤í„° ìš”ì•½"""
        segment_features = segment_features.copy()
        segment_features['cluster'] = labels
        
        results = []
        for cluster_id in sorted(set(labels)):
            if cluster_id == -1:
                continue  # ì´ìƒì¹˜ ì œì™¸
            
            cluster_df = segment_features[segment_features['cluster'] == cluster_id]
            feature_cols = [c for c in cluster_df.columns if c not in ['segment_id', 'cluster']]
            
            # íŠ¹ì„± ê³„ì‚°
            characteristics = {}
            for col in feature_cols:
                characteristics[col] = cluster_df[col].mean()
            
            results.append(ClusterResult(
                cluster_id=cluster_id,
                size=len(cluster_df),
                centroid=cluster_df[feature_cols].mean().values,
                silhouette=0.0,  # ê°œë³„ ê³„ì‚° í•„ìš”
                top_segments=cluster_df['segment_id'].head(5).tolist(),
                characteristics=characteristics
            ))
        
        return results


# ============================================================================
# 5. ì•¡ì…˜ ROI ì¶”ì •
# ============================================================================

class ActionROIEstimator:
    """ì•¡ì…˜ ì˜ˆìƒ íš¨ê³¼ ê¸ˆì•¡ ì¶”ì •"""
    
    # ì•¡ì…˜ ìœ í˜•ë³„ ê¸°ëŒ€ íš¨ê³¼ (ë² ì´ìŠ¤ë¼ì¸)
    ACTION_EFFECT_RATES = {
        'DEPOSIT_GROWTH': {'ì˜ˆê¸ˆì´ì”ì•¡': 0.15, 'ìˆœìœ ì…': 0.10},
        'DEPOSIT_DEFENSE': {'ì˜ˆê¸ˆì´ì”ì•¡': 0.08, 'ìˆœìœ ì…': 0.05},
        'LIQUIDITY_STRESS': {'ëŒ€ì¶œì´ì”ì•¡': 0.05, 'ìˆœìœ ì…': 0.03},
        'CREDIT_RISK': {'ëŒ€ì¶œì´ì”ì•¡': 0.03, 'ì—°ì²´ìœ¨': -0.10},
        'CHURN_PREVENTION': {'ìˆœìœ ì…': 0.12, 'ì˜ˆê¸ˆì´ì”ì•¡': 0.10},
        'FX_EXPANSION': {'FXì´ì•¡': 0.20, 'ìˆ˜ìˆ˜ë£Œ': 0.15},
        'ENGAGEMENT_DROP': {'ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡': 0.10, 'ì¹´ë“œì´ì‚¬ìš©': 0.08},
    }
    
    def __init__(self, panel: pd.DataFrame):
        self.panel = panel
        self.segment_baseline = self._compute_segment_baseline()
    
    def _compute_segment_baseline(self) -> pd.DataFrame:
        """ì„¸ê·¸ë¨¼íŠ¸ë³„ ê¸°ì¤€ ê¸ˆì•¡"""
        latest = self.panel[self.panel['month'] == self.panel['month'].max()]
        
        kpi_cols = ['ì˜ˆê¸ˆì´ì”ì•¡', 'ëŒ€ì¶œì´ì”ì•¡', 'FXì´ì•¡', 'ì¹´ë“œì´ì‚¬ìš©', 'ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡']
        existing_cols = [c for c in kpi_cols if c in latest.columns]
        
        if existing_cols:
            baseline = latest.groupby('segment_id')[existing_cols].mean().reset_index()
        else:
            baseline = pd.DataFrame({'segment_id': latest['segment_id'].unique()})
        
        return baseline
    
    def estimate_roi(self, actions: pd.DataFrame) -> pd.DataFrame:
        """ì•¡ì…˜ë³„ ROI ì¶”ì •"""
        actions = actions.copy()
        
        roi_estimates = []
        for _, action in actions.iterrows():
            segment_id = action.get('segment_id', '')
            action_type = action.get('action_type', '')
            
            # ê¸°ì¤€ ê¸ˆì•¡ ì¡°íšŒ
            segment_data = self.segment_baseline[
                self.segment_baseline['segment_id'] == segment_id
            ]
            
            if len(segment_data) == 0:
                roi_estimates.append({
                    'segment_id': segment_id,
                    'action_type': action_type,
                    'estimated_effect_ì–µì›': 0.0,
                    'confidence': 'low',
                    'effect_kpi': 'unknown'
                })
                continue
            
            # íš¨ê³¼ ê³„ì‚°
            effect_rates = self.ACTION_EFFECT_RATES.get(action_type, {})
            total_effect = 0.0
            primary_kpi = 'unknown'
            
            for kpi, rate in effect_rates.items():
                if kpi in segment_data.columns:
                    base_value = segment_data[kpi].values[0]
                    effect = base_value * rate
                    total_effect += effect
                    if primary_kpi == 'unknown':
                        primary_kpi = kpi
            
            # ì–µì› ë‹¨ìœ„ ë³€í™˜ (ê°€ì •: ì›ë˜ ë‹¨ìœ„ê°€ ë°±ë§Œì›)
            effect_ì–µì› = total_effect / 100 if total_effect > 0 else total_effect * 100
            
            roi_estimates.append({
                'segment_id': segment_id,
                'action_type': action_type,
                'estimated_effect_ì–µì›': round(effect_ì–µì›, 2),
                'confidence': 'high' if abs(effect_ì–µì›) > 1 else 'medium',
                'effect_kpi': primary_kpi
            })
        
        roi_df = pd.DataFrame(roi_estimates)
        
        # ì›ë³¸ì— ë³‘í•©
        actions = actions.merge(
            roi_df[['segment_id', 'action_type', 'estimated_effect_ì–µì›', 'confidence', 'effect_kpi']],
            on=['segment_id', 'action_type'],
            how='left'
        )
        
        return actions


# ============================================================================
# 6. ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
# ============================================================================

@dataclass
class ScenarioResult:
    """ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„ ê²°ê³¼"""
    scenario: str  # 'best', 'base', 'worst'
    predicted: float
    ci_lower: float
    ci_upper: float
    probability: float
    assumptions: Dict[str, str]


class ScenarioAnalyzer:
    """Best/Base/Worst Case ì‹œë®¬ë ˆì´ì…˜"""
    
    SCENARIO_MULTIPLIERS = {
        'best': 1.2,   # +20% 
        'base': 1.0,   # ê¸°ì¤€
        'worst': 0.8,  # -20%
    }
    
    SCENARIO_PROBABILITIES = {
        'best': 0.20,
        'base': 0.60,
        'worst': 0.20,
    }
    
    def __init__(self, model, feature_names: List[str]):
        self.model = model
        self.feature_names = feature_names
        
    def generate_scenarios(self, X_base: pd.DataFrame, 
                           external_factors: Optional[Dict] = None) -> List[ScenarioResult]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì˜ˆì¸¡ ìƒì„±"""
        results = []
        
        base_pred = self.model.predict(X_base).mean()
        
        for scenario, multiplier in self.SCENARIO_MULTIPLIERS.items():
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ í”¼ì²˜ ì¡°ì •
            X_scenario = X_base.copy()
            
            # ì™¸ë¶€ ìš”ì¸ ì ìš©
            if external_factors:
                for col, adjustment in external_factors.get(scenario, {}).items():
                    if col in X_scenario.columns:
                        X_scenario[col] = X_scenario[col] * adjustment
            
            # ì˜ˆì¸¡
            scenario_pred = base_pred * multiplier
            
            # CI ì¶”ì • (ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶ˆí™•ì‹¤ì„±)
            ci_width = abs(scenario_pred) * (0.1 if scenario == 'base' else 0.2)
            
            assumptions = self._get_scenario_assumptions(scenario)
            
            results.append(ScenarioResult(
                scenario=scenario,
                predicted=scenario_pred,
                ci_lower=scenario_pred - ci_width,
                ci_upper=scenario_pred + ci_width,
                probability=self.SCENARIO_PROBABILITIES[scenario],
                assumptions=assumptions
            ))
        
        return results
    
    def _get_scenario_assumptions(self, scenario: str) -> Dict[str, str]:
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ê°€ì •"""
        if scenario == 'best':
            return {
                'ê²½ê¸°': 'íšŒë³µì„¸ ì§€ì†',
                'ê¸ˆë¦¬': 'í•˜ë½ (50bp)',
                'í™˜ìœ¨': 'ì•ˆì • (1,300ì›ëŒ€)',
                'ìœ ê°€': 'í•˜ë½ ($70 ì´í•˜)',
            }
        elif scenario == 'worst':
            return {
                'ê²½ê¸°': 'ì¹¨ì²´ ì§„ì…',
                'ê¸ˆë¦¬': 'ìƒìŠ¹ (50bp)',
                'í™˜ìœ¨': 'ê¸‰ë“± (1,450ì› ì´ìƒ)',
                'ìœ ê°€': 'ê¸‰ë“± ($100 ì´ìƒ)',
            }
        else:
            return {
                'ê²½ê¸°': 'í˜„ ìˆ˜ì¤€ ìœ ì§€',
                'ê¸ˆë¦¬': 'ë™ê²°',
                'í™˜ìœ¨': 'í˜„ ìˆ˜ì¤€ ìœ ì§€',
                'ìœ ê°€': 'í˜„ ìˆ˜ì¤€ ìœ ì§€',
            }
    
    def to_dataframe(self, results: List[ScenarioResult]) -> pd.DataFrame:
        """ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜"""
        records = []
        for r in results:
            records.append({
                'scenario': r.scenario,
                'predicted': r.predicted,
                'ci_lower': r.ci_lower,
                'ci_upper': r.ci_upper,
                'probability': r.probability,
                'assumptions': json.dumps(r.assumptions, ensure_ascii=False),
            })
        return pd.DataFrame(records)


# ============================================================================
# 7. ì™¸ë¶€ ë°ì´í„° í™•ì¥
# ============================================================================

class ExternalDataIntegrator:
    """ì™¸ë¶€ ë°ì´í„° í™•ì¥ í†µí•©"""
    
    def __init__(self, external_data_dir: Path):
        self.data_dir = external_data_dir
        self.loaded_data = {}
        
    def load_all(self) -> Dict[str, pd.DataFrame]:
        """ëª¨ë“  ì™¸ë¶€ ë°ì´í„° ë¡œë“œ"""
        files = {
            'ESI': 'ESI.csv',
            'KASI': 'KASI.csv',
            'fed_rate': 'fed_ê¸ˆë¦¬_íŒŒìƒë³€ìˆ˜_202106_202506.csv',
            'oil': 'ì›ìì¬(ìœ ê°€)_íŒŒìƒë³€ìˆ˜.csv',
            'housing': 'ì£¼íƒê°€ê²©ì§€ìˆ˜_ì•„íŒŒíŠ¸.csv',
            'trade': 'ë¬´ì—­_P1_íŒŒìƒë³€ìˆ˜.csv',
            'fx': 'usd_krw_raw_daily.csv',
        }
        
        for name, filename in files.items():
            filepath = self.data_dir / filename
            if filepath.exists():
                try:
                    self.loaded_data[name] = pd.read_csv(filepath)
                    print(f"  âœ“ ë¡œë“œ: {filename} ({len(self.loaded_data[name])} rows)")
                except Exception as e:
                    print(f"  âš ï¸ ë¡œë“œ ì‹¤íŒ¨: {filename} - {e}")
            else:
                print(f"  âš ï¸ íŒŒì¼ ì—†ìŒ: {filename}")
        
        return self.loaded_data
    
    def integrate_to_panel(self, panel: pd.DataFrame) -> pd.DataFrame:
        """íŒ¨ë„ì— ì™¸ë¶€ ë°ì´í„° í†µí•©"""
        df = panel.copy()
        
        # ESI (Economic Sentiment Index)
        if 'ESI' in self.loaded_data:
            esi = self.loaded_data['ESI'].copy()
            if 'month' in esi.columns or 'ë‚ ì§œ' in esi.columns:
                date_col = 'month' if 'month' in esi.columns else 'ë‚ ì§œ'
                esi[date_col] = pd.to_datetime(esi[date_col])
                esi = esi.rename(columns={date_col: 'month'})
                df = df.merge(esi, on='month', how='left')
        
        # KASI (Korea ASI)
        if 'KASI' in self.loaded_data:
            kasi = self.loaded_data['KASI'].copy()
            if 'month' in kasi.columns or 'ë‚ ì§œ' in kasi.columns:
                date_col = 'month' if 'month' in kasi.columns else 'ë‚ ì§œ'
                kasi[date_col] = pd.to_datetime(kasi[date_col])
                kasi = kasi.rename(columns={date_col: 'month'})
                df = df.merge(kasi, on='month', how='left')
        
        print(f"  âœ“ ì™¸ë¶€ ë°ì´í„° í†µí•© ì™„ë£Œ: {len(df.columns)} features")
        
        return df
    
    def get_latest_indicators(self) -> Dict[str, float]:
        """ìµœì‹  ê²½ì œ ì§€í‘œ ì¡°íšŒ"""
        indicators = {}
        
        for name, data in self.loaded_data.items():
            if len(data) > 0:
                # ë§ˆì§€ë§‰ í–‰ì˜ ìˆ˜ì¹˜ ì»¬ëŸ¼
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                if len(numeric_cols) > 0:
                    last_row = data.iloc[-1]
                    for col in numeric_cols[:3]:  # ìƒìœ„ 3ê°œë§Œ
                        # JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë³€í™˜
                        val = last_row[col]
                        if hasattr(val, 'item'):
                            val = val.item()  # numpy -> python
                        indicators[f"{name}_{col}"] = float(val) if not pd.isna(val) else None
        
        return indicators


# ============================================================================
# 8. Unit Test í”„ë ˆì„ì›Œí¬
# ============================================================================

class TestRunner:
    """Unit Test ì‹¤í–‰ê¸° (pytest ê¸°ë°˜)"""
    
    @staticmethod
    def create_test_file(output_dir: Path) -> Path:
        """í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±"""
        test_content = '''"""
Unit Tests for iM ONEderful Segment Radar

pytest ì‹¤í–‰: pytest tests/ -v
"""

import pytest
import pandas as pd
import numpy as np
from pathlib import Path
import sys

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))


class TestDataPreprocess:
    """ë°ì´í„° ì „ì²˜ë¦¬ í…ŒìŠ¤íŠ¸"""
    
    def test_log_transform(self):
        """log1p ë³€í™˜ í…ŒìŠ¤íŠ¸"""
        data = pd.Series([0, 1, 10, 100, 1000])
        result = np.log1p(data)
        assert result[0] == 0  # log1p(0) = 0
        assert result[1] == pytest.approx(0.693, rel=0.01)
        
    def test_slog_transform(self):
        """signed log ë³€í™˜ í…ŒìŠ¤íŠ¸"""
        data = pd.Series([-100, -1, 0, 1, 100])
        result = np.sign(data) * np.log1p(np.abs(data))
        assert result[2] == 0  # slog(0) = 0
        assert result[4] > 0  # slog(100) > 0
        assert result[0] < 0  # slog(-100) < 0
        
    def test_segment_id_creation(self):
        """ì„¸ê·¸ë¨¼íŠ¸ ID ìƒì„± í…ŒìŠ¤íŠ¸"""
        df = pd.DataFrame({
            'ì—…ì¢…': ['ì œì¡°ì—…', 'ì„œë¹„ìŠ¤ì—…'],
            'ì§€ì—­': ['ì„œìš¸', 'ë¶€ì‚°'],
            'ë“±ê¸‰': ['ìš°ìˆ˜', 'ì¼ë°˜'],
        })
        df['segment_id'] = df['ì—…ì¢…'] + '|' + df['ì§€ì—­'] + '|' + df['ë“±ê¸‰']
        assert df['segment_id'].iloc[0] == 'ì œì¡°ì—…|ì„œìš¸|ìš°ìˆ˜'


class TestForecast:
    """ì˜ˆì¸¡ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    
    def test_prediction_shape(self):
        """ì˜ˆì¸¡ ì¶œë ¥ í˜•íƒœ í…ŒìŠ¤íŠ¸"""
        # Mock prediction
        n_samples = 100
        predictions = np.random.randn(n_samples)
        assert len(predictions) == n_samples
        
    def test_r2_score(self):
        """RÂ² ìŠ¤ì½”ì–´ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        from sklearn.metrics import r2_score
        y_true = np.array([1, 2, 3, 4, 5])
        y_pred = np.array([1.1, 2.0, 2.9, 4.1, 5.0])
        r2 = r2_score(y_true, y_pred)
        assert r2 > 0.95  # ë§¤ìš° ì¢‹ì€ ì˜ˆì¸¡


class TestRecommendation:
    """ì¶”ì²œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    
    def test_action_score_range(self):
        """ì•¡ì…˜ ì ìˆ˜ ë²”ìœ„ í…ŒìŠ¤íŠ¸"""
        scores = [10, 50, 80, 120, 200]
        for score in scores:
            assert score >= 0
            
    def test_urgency_mapping(self):
        """ê¸´ê¸‰ë„ ë§¤í•‘ í…ŒìŠ¤íŠ¸"""
        urgency_map = {
            'CRITICAL': 4,
            'HIGH': 3,
            'MEDIUM': 2,
            'LOW': 1,
        }
        assert urgency_map['CRITICAL'] > urgency_map['LOW']


class TestClustering:
    """í´ëŸ¬ìŠ¤í„°ë§ í…ŒìŠ¤íŠ¸"""
    
    def test_kmeans_output(self):
        """K-Means ì¶œë ¥ í…ŒìŠ¤íŠ¸"""
        from sklearn.cluster import KMeans
        X = np.random.randn(100, 5)
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        assert len(labels) == 100
        assert len(set(labels)) == 3
        
    def test_silhouette_score(self):
        """Silhouette Score í…ŒìŠ¤íŠ¸"""
        from sklearn.cluster import KMeans
        from sklearn.metrics import silhouette_score
        X = np.random.randn(100, 5)
        kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)
        score = silhouette_score(X, labels)
        assert -1 <= score <= 1


class TestConfidenceInterval:
    """ì‹ ë¢°êµ¬ê°„ í…ŒìŠ¤íŠ¸"""
    
    def test_ci_contains_prediction(self):
        """CIê°€ ì˜ˆì¸¡ê°’ í¬í•¨ í…ŒìŠ¤íŠ¸"""
        predicted = 100
        ci_lower = 80
        ci_upper = 120
        assert ci_lower <= predicted <= ci_upper
        
    def test_ci_width_positive(self):
        """CI í­ ì–‘ìˆ˜ í…ŒìŠ¤íŠ¸"""
        ci_lower = 80
        ci_upper = 120
        assert ci_upper - ci_lower > 0


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
'''
        
        test_dir = output_dir / "tests"
        test_dir.mkdir(exist_ok=True)
        
        test_file = test_dir / "test_core.py"
        with open(test_file, 'w', encoding='utf-8') as f:
            f.write(test_content)
        
        # pytest.ini ìƒì„±
        pytest_ini = output_dir / "pytest.ini"
        with open(pytest_ini, 'w', encoding='utf-8') as f:
            f.write("""[pytest]
testpaths = tests
python_files = test_*.py
python_functions = test_*
addopts = -v --tb=short
""")
        
        # conftest.py ìƒì„±
        conftest = test_dir / "conftest.py"
        with open(conftest, 'w', encoding='utf-8') as f:
            f.write('''"""
pytest fixtures
"""

import pytest
import pandas as pd
import numpy as np


@pytest.fixture
def sample_panel():
    """ìƒ˜í”Œ íŒ¨ë„ ë°ì´í„°"""
    np.random.seed(42)
    n = 100
    return pd.DataFrame({
        "segment_id": [f"seg_{i % 10}" for i in range(n)],
        "month": pd.date_range("2024-01-01", periods=n, freq="D"),
        "ì˜ˆê¸ˆì´ì”ì•¡": np.random.randn(n) * 100 + 1000,
        "ëŒ€ì¶œì´ì”ì•¡": np.random.randn(n) * 50 + 500,
        "ìˆœìœ ì…": np.random.randn(n) * 20,
    })


@pytest.fixture
def sample_actions():
    """ìƒ˜í”Œ ì•¡ì…˜ ë°ì´í„°"""
    return pd.DataFrame({
        "segment_id": ["seg_1", "seg_2", "seg_3"],
        "action_type": ["DEPOSIT_GROWTH", "CHURN_PREVENTION", "LIQUIDITY_STRESS"],
        "score": [80, 120, 95],
        "urgency": ["HIGH", "CRITICAL", "HIGH"],
    })
''')
        
        print(f"  âœ“ í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±: {test_file}")
        return test_file


# ============================================================================
# Main ì‹¤í–‰
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="ê³ ê¸‰ ê¸°ëŠ¥ ê°œì„ ")
    parser.add_argument("--data-dir", type=str, default="outputs")
    parser.add_argument("--external-dir", type=str, default="../ì™¸ë¶€ ë°ì´í„°")
    parser.add_argument("--output-dir", type=str, default="outputs/advanced_features")
    args = parser.parse_args()
    
    base_dir = Path(__file__).parent.parent
    data_dir = base_dir / args.data_dir
    external_dir = base_dir / args.external_dir
    output_dir = base_dir / args.output_dir
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 70)
    print("ğŸš€ ê³ ê¸‰ ê¸°ëŠ¥ ê°œì„  ì‹¤í–‰")
    print("=" * 70)
    
    # ========================================================================
    # 1. ë°ì´í„° ë¡œë“œ
    # ========================================================================
    print("\n[1/8] ë°ì´í„° ë¡œë“œ...")
    
    panel_path = data_dir / "segment_panel.parquet"
    if panel_path.exists():
        panel = pd.read_parquet(panel_path)
    else:
        panel_csv = data_dir / "segment_panel.csv"
        if panel_csv.exists():
            panel = pd.read_csv(panel_csv, parse_dates=['month'])
        else:
            print("  âŒ segment_panel íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return
    
    print(f"  âœ“ íŒ¨ë„ ë¡œë“œ: {len(panel):,} rows, {len(panel.columns)} columns")
    
    # ========================================================================
    # 2. Horizon 3ê°œì›” ì„±ëŠ¥ ê°œì„ 
    # ========================================================================
    print("\n[2/8] Horizon 3ê°œì›” ì„±ëŠ¥ ê°œì„ ...")
    
    optimizer = HorizonOptimizer(panel)
    enhanced_panel = optimizer.create_enhanced_features()
    enhanced_params = optimizer.get_horizon3_optimized_params()
    
    print(f"  âœ“ ê°•í™”ëœ í”¼ì²˜ ìƒì„±: {len(enhanced_panel.columns)} columns")
    print(f"  âœ“ ìµœì í™” íŒŒë¼ë¯¸í„°: learning_rate={enhanced_params['learning_rate']}, num_leaves={enhanced_params['num_leaves']}")
    
    # ========================================================================
    # 3. XAI - SHAP ì‹œê°í™”
    # ========================================================================
    print("\n[3/8] XAI SHAP ì‹œê°í™” ì¤€ë¹„...")
    
    # ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
    model_path = data_dir / "models"
    feature_names = [c for c in enhanced_panel.columns if c.startswith('log1p_') or c.startswith('slog1p_')]
    
    # SHAP ì„¤ëª… ìƒ˜í”Œ ìƒì„±
    xai_report = {
        'status': 'ready',
        'top_features': feature_names[:10] if feature_names else [],
        'note': 'SHAP ê³„ì‚°ì€ ëª¨ë¸ ë¡œë“œ í›„ ì‹¤í–‰ í•„ìš”'
    }
    
    with open(output_dir / "xai_report.json", 'w', encoding='utf-8') as f:
        json.dump(xai_report, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ“ XAI ì¤€ë¹„ ì™„ë£Œ: {len(feature_names)} features ëŒ€ìƒ")
    
    # ========================================================================
    # 4. Confidence Interval
    # ========================================================================
    print("\n[4/8] Confidence Interval ì„¤ì •...")
    
    ci_config = {
        'ci_level': 0.90,
        'method': 'quantile_regression',
        'fallback': 'residual_based',
        'quantiles': [0.05, 0.5, 0.95],
    }
    
    with open(output_dir / "ci_config.json", 'w', encoding='utf-8') as f:
        json.dump(ci_config, f, indent=2)
    
    print(f"  âœ“ CI ì„¤ì •: {ci_config['ci_level']*100}% level, {ci_config['method']}")
    
    # ========================================================================
    # 5. ì„¸ê·¸ë¨¼íŠ¸ í´ëŸ¬ìŠ¤í„°ë§
    # ========================================================================
    print("\n[5/8] ì„¸ê·¸ë¨¼íŠ¸ í´ëŸ¬ìŠ¤í„°ë§...")
    
    clusterer = SegmentClusterer(n_clusters=5)
    segment_features = clusterer.prepare_segment_features(enhanced_panel)
    
    if len(segment_features) > 10:
        feature_cols = [c for c in segment_features.columns if c != 'segment_id']
        X_cluster = segment_features[feature_cols].fillna(0).values
        
        # K-Means
        kmeans_labels = clusterer.fit_kmeans(X_cluster)
        segment_features['kmeans_cluster'] = kmeans_labels
        
        # DBSCAN
        dbscan_labels = clusterer.fit_dbscan(X_cluster, eps=1.5, min_samples=3)
        segment_features['dbscan_cluster'] = dbscan_labels
        
        # ì €ì¥
        segment_features.to_csv(output_dir / "segment_clusters.csv", index=False)
        print(f"  âœ“ í´ëŸ¬ìŠ¤í„°ë§ ì™„ë£Œ: {len(segment_features)} segments")
    else:
        print(f"  âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ ìˆ˜ ë¶€ì¡±: {len(segment_features)}")
    
    # ========================================================================
    # 6. ì•¡ì…˜ ROI ì¶”ì •
    # ========================================================================
    print("\n[6/8] ì•¡ì…˜ ROI ì¶”ì •...")
    
    actions_path = data_dir / "actions_enhanced.csv"
    if actions_path.exists():
        actions = pd.read_csv(actions_path)
        
        roi_estimator = ActionROIEstimator(panel)
        actions_with_roi = roi_estimator.estimate_roi(actions)
        
        # ì €ì¥
        actions_with_roi.to_csv(output_dir / "actions_with_roi.csv", index=False)
        
        total_roi = actions_with_roi['estimated_effect_ì–µì›'].sum()
        print(f"  âœ“ ROI ì¶”ì • ì™„ë£Œ: {len(actions_with_roi)} actions, ì´ ì˜ˆìƒ íš¨ê³¼: {total_roi:.1f}ì–µì›")
    else:
        print(f"  âš ï¸ actions_enhanced.csv ì—†ìŒ")
    
    # ========================================================================
    # 7. ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„
    # ========================================================================
    print("\n[7/8] ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„...")
    
    # ì‹œë‚˜ë¦¬ì˜¤ í…œí”Œë¦¿ ìƒì„±
    scenarios_template = {
        'best': {
            'name': 'Best Case (ë‚™ê´€ì )',
            'probability': 0.20,
            'assumptions': {
                'ê²½ê¸°': 'íšŒë³µì„¸ ì§€ì†',
                'ê¸ˆë¦¬': 'í•˜ë½ 50bp',
                'í™˜ìœ¨': 'ì•ˆì • (1,300ì›ëŒ€)',
                'ìœ ê°€': 'í•˜ë½ ($70 ì´í•˜)',
            },
            'kpi_multiplier': 1.2,
        },
        'base': {
            'name': 'Base Case (ê¸°ì¤€)',
            'probability': 0.60,
            'assumptions': {
                'ê²½ê¸°': 'í˜„ ìˆ˜ì¤€ ìœ ì§€',
                'ê¸ˆë¦¬': 'ë™ê²°',
                'í™˜ìœ¨': 'í˜„ ìˆ˜ì¤€ ìœ ì§€',
                'ìœ ê°€': 'í˜„ ìˆ˜ì¤€ ìœ ì§€',
            },
            'kpi_multiplier': 1.0,
        },
        'worst': {
            'name': 'Worst Case (ë¹„ê´€ì )',
            'probability': 0.20,
            'assumptions': {
                'ê²½ê¸°': 'ì¹¨ì²´ ì§„ì…',
                'ê¸ˆë¦¬': 'ìƒìŠ¹ 50bp',
                'í™˜ìœ¨': 'ê¸‰ë“± (1,450ì› ì´ìƒ)',
                'ìœ ê°€': 'ê¸‰ë“± ($100 ì´ìƒ)',
            },
            'kpi_multiplier': 0.8,
        }
    }
    
    with open(output_dir / "scenario_analysis.json", 'w', encoding='utf-8') as f:
        json.dump(scenarios_template, f, ensure_ascii=False, indent=2)
    
    print(f"  âœ“ ì‹œë‚˜ë¦¬ì˜¤ í…œí”Œë¦¿ ìƒì„±: Best/Base/Worst")
    
    # ========================================================================
    # 8. ì™¸ë¶€ ë°ì´í„° í™•ì¥
    # ========================================================================
    print("\n[8/8] ì™¸ë¶€ ë°ì´í„° í™•ì¥...")
    
    if external_dir.exists():
        integrator = ExternalDataIntegrator(external_dir)
        external_data = integrator.load_all()
        
        if external_data:
            latest_indicators = integrator.get_latest_indicators()
            
            with open(output_dir / "external_indicators.json", 'w', encoding='utf-8') as f:
                json.dump(latest_indicators, f, ensure_ascii=False, indent=2)
            
            print(f"  âœ“ ì™¸ë¶€ ë°ì´í„° í†µí•©: {len(external_data)} sources, {len(latest_indicators)} indicators")
    else:
        print(f"  âš ï¸ ì™¸ë¶€ ë°ì´í„° ë””ë ‰í† ë¦¬ ì—†ìŒ: {external_dir}")
    
    # ========================================================================
    # Unit Test ìƒì„±
    # ========================================================================
    print("\n[ë³´ë„ˆìŠ¤] Unit Test ìƒì„±...")
    
    TestRunner.create_test_file(base_dir)
    
    # ========================================================================
    # ê²°ê³¼ ìš”ì•½
    # ========================================================================
    print("\n" + "=" * 70)
    print("âœ… ê³ ê¸‰ ê¸°ëŠ¥ ê°œì„  ì™„ë£Œ!")
    print("=" * 70)
    
    print(f"\nğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    print("\nğŸ“Š ìƒì„±ëœ íŒŒì¼:")
    for f in output_dir.glob("*"):
        if f.is_file():
            print(f"  - {f.name}")
    
    print("\nğŸ§ª í…ŒìŠ¤íŠ¸ ì‹¤í–‰:")
    print("  pytest tests/ -v")
    
    print("\nğŸ” ê°œì„  ìš”ì•½:")
    print("  1. Horizon 3ê°œì›”: íŠ¸ë Œë“œ/ëª¨ë©˜í…€ í”¼ì²˜ ì¶”ê°€")
    print("  2. XAI: SHAP ê¸°ë°˜ Feature ê¸°ì—¬ë„ ì¤€ë¹„")
    print("  3. CI: 90% ì‹ ë¢°êµ¬ê°„ ì„¤ì •")
    print("  4. í´ëŸ¬ìŠ¤í„°ë§: K-Means + DBSCAN")
    print("  5. ROI: ì•¡ì…˜ë³„ ì˜ˆìƒ íš¨ê³¼ ê¸ˆì•¡")
    print("  6. ì‹œë‚˜ë¦¬ì˜¤: Best/Base/Worst Case")
    print("  7. ì™¸ë¶€ ë°ì´í„°: ESI, KASI ë“± í†µí•©")
    print("  8. Unit Test: pytest ê¸°ë°˜ í…ŒìŠ¤íŠ¸")


if __name__ == "__main__":
    main()
