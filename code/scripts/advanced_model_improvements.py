#!/usr/bin/env python3
"""
ê³ ê¸‰ ëª¨ë¸ ê°œì„  ìŠ¤í¬ë¦½íŠ¸

ê°œì„  ì‚¬í•­:
1. ìˆœìœ ì… ë¶€í˜¸ ë¶„ë¥˜ ëª¨ë¸ (Two-Stage)
2. Stacking ì•™ìƒë¸”
3. ì‹œê³„ì—´ êµì°¨ê²€ì¦ ê°•í™”
"""

import sys
import os
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì„¤ì • (run_mvp.pyì™€ ë™ì¼í•œ ë°©ì‹)
REPO_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
SRC_ROOT = os.path.join(REPO_ROOT, "src")
sys.path.insert(0, SRC_ROOT)

import warnings
warnings.filterwarnings('ignore')

import argparse
import numpy as np
import pandas as pd
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor, CatBoostClassifier
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, f1_score, classification_report
)
import optuna
from optuna.samplers import TPESampler
optuna.logging.set_verbosity(optuna.logging.WARNING)

from imradar.config import RadarConfig
from imradar.data.io import load_internal_csv
from imradar.data.preprocess import aggregate_to_segment_month, build_full_panel
from imradar.features.kpi import compute_kpis
from imradar.features.lags import add_lag_features, add_rolling_features, add_change_features
from imradar.data.external import load_all_external_data, merge_external_to_panel, add_macro_lag_features


# ============================================================================
# 1. ìˆœìœ ì… ë¶€í˜¸ ë¶„ë¥˜ ëª¨ë¸ (Two-Stage)
# ============================================================================

class TwoStageNetFlowModel:
    """
    ìˆœìœ ì… Two-Stage ëª¨ë¸:
    Stage 1: ë¶€í˜¸ ì˜ˆì¸¡ (ë¶„ë¥˜) - ìˆœìœ ì…ì´ +ì¸ì§€ -ì¸ì§€
    Stage 2: í¬ê¸° ì˜ˆì¸¡ (íšŒê·€) - ì ˆëŒ€ê°’ í¬ê¸° ì˜ˆì¸¡
    ìµœì¢… ì˜ˆì¸¡ = ë¶€í˜¸ * í¬ê¸°
    """
    
    def __init__(self, horizon: int = 1):
        self.horizon = horizon
        self.sign_model = None  # ë¶„ë¥˜ ëª¨ë¸
        self.magnitude_model_pos = None  # ì–‘ìˆ˜ í¬ê¸° ëª¨ë¸
        self.magnitude_model_neg = None  # ìŒìˆ˜ í¬ê¸° ëª¨ë¸
        self.feature_cols = None
        
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
            categorical_cols: list = None, verbose: bool = True):
        """Two-Stage ëª¨ë¸ í•™ìŠµ"""
        
        self.feature_cols = list(X_train.columns)
        
        # Stage 1: ë¶€í˜¸ ë¶„ë¥˜ ëª¨ë¸
        y_sign = (y_train > 0).astype(int)  # 1: ì–‘ìˆ˜, 0: ìŒìˆ˜
        
        if verbose:
            print(f"    [Stage 1] ë¶€í˜¸ ë¶„ë¥˜ ëª¨ë¸ í•™ìŠµ...")
            print(f"      - ì–‘ìˆ˜ ë¹„ìœ¨: {y_sign.mean():.1%}")
        
        self.sign_model = CatBoostClassifier(
            iterations=300,
            learning_rate=0.05,
            depth=6,
            loss_function='Logloss',
            verbose=False,
            random_seed=42
        )
        
        cat_features = [X_train.columns.get_loc(c) for c in (categorical_cols or []) 
                       if c in X_train.columns]
        self.sign_model.fit(X_train, y_sign, cat_features=cat_features if cat_features else None)
        
        # Stage 1 ì„±ëŠ¥
        sign_pred = self.sign_model.predict(X_train)
        sign_acc = accuracy_score(y_sign, sign_pred)
        if verbose:
            print(f"      - ë¶€í˜¸ ì˜ˆì¸¡ ì •í™•ë„: {sign_acc:.1%}")
        
        # Stage 2: í¬ê¸° íšŒê·€ ëª¨ë¸ (ì–‘ìˆ˜/ìŒìˆ˜ ë¶„ë¦¬)
        if verbose:
            print(f"    [Stage 2] í¬ê¸° ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ...")
        
        # ì–‘ìˆ˜ ë°ì´í„°
        pos_mask = y_train > 0
        X_pos = X_train[pos_mask]
        y_pos = np.abs(y_train[pos_mask])
        
        # ìŒìˆ˜ ë°ì´í„°
        neg_mask = y_train < 0
        X_neg = X_train[neg_mask]
        y_neg = np.abs(y_train[neg_mask])
        
        if verbose:
            print(f"      - ì–‘ìˆ˜ ìƒ˜í”Œ: {len(y_pos):,}, ìŒìˆ˜ ìƒ˜í”Œ: {len(y_neg):,}")
        
        # ì–‘ìˆ˜ í¬ê¸° ëª¨ë¸
        if len(y_pos) > 100:
            self.magnitude_model_pos = lgb.LGBMRegressor(
                n_estimators=300,
                learning_rate=0.05,
                max_depth=8,
                num_leaves=64,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                verbose=-1,
                random_state=42
            )
            self.magnitude_model_pos.fit(X_pos, y_pos)
        
        # ìŒìˆ˜ í¬ê¸° ëª¨ë¸
        if len(y_neg) > 100:
            self.magnitude_model_neg = lgb.LGBMRegressor(
                n_estimators=300,
                learning_rate=0.05,
                max_depth=8,
                num_leaves=64,
                min_child_samples=20,
                subsample=0.8,
                colsample_bytree=0.8,
                reg_alpha=0.1,
                reg_lambda=0.1,
                verbose=-1,
                random_state=42
            )
            self.magnitude_model_neg.fit(X_neg, y_neg)
        
        return self
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Two-Stage ì˜ˆì¸¡"""
        # Stage 1: ë¶€í˜¸ ì˜ˆì¸¡
        sign_prob = self.sign_model.predict_proba(X)[:, 1]  # ì–‘ìˆ˜ í™•ë¥ 
        sign_pred = (sign_prob > 0.5).astype(int)
        
        # Stage 2: í¬ê¸° ì˜ˆì¸¡
        magnitude = np.zeros(len(X))
        
        pos_mask = sign_pred == 1
        neg_mask = sign_pred == 0
        
        if self.magnitude_model_pos is not None and pos_mask.sum() > 0:
            magnitude[pos_mask] = self.magnitude_model_pos.predict(X[pos_mask])
        
        if self.magnitude_model_neg is not None and neg_mask.sum() > 0:
            magnitude[neg_mask] = self.magnitude_model_neg.predict(X[neg_mask])
        
        # ìµœì¢… ì˜ˆì¸¡: ë¶€í˜¸ * í¬ê¸°
        final_pred = np.where(sign_pred == 1, magnitude, -magnitude)
        
        # Soft prediction (í™•ë¥  ê¸°ë°˜)
        # ë¶€í˜¸ í™•ë¥ ë¡œ ê°€ì¤‘ í‰ê· 
        soft_pred = sign_prob * magnitude + (1 - sign_prob) * (-magnitude)
        
        return soft_pred


# ============================================================================
# 2. Stacking ì•™ìƒë¸”
# ============================================================================

class StackingEnsemble:
    """
    Stacking ì•™ìƒë¸”:
    Level 1: LightGBM, XGBoost, CatBoost (ê¸°ë³¸ ëª¨ë¸)
    Level 2: Ridge (ë©”íƒ€ ëª¨ë¸)
    """
    
    def __init__(self):
        self.base_models = {}
        self.meta_model = None
        self.feature_cols = None
        
    def fit(self, X_train: pd.DataFrame, y_train: pd.Series,
            X_val: pd.DataFrame = None, y_val: pd.Series = None,
            categorical_cols: list = None, n_folds: int = 3,
            verbose: bool = True):
        """Stacking í•™ìŠµ"""
        
        self.feature_cols = list(X_train.columns)
        
        if verbose:
            print(f"    [Level 1] ê¸°ë³¸ ëª¨ë¸ í•™ìŠµ ({n_folds}-Fold CV)...")
        
        # Level 1: ê¸°ë³¸ ëª¨ë¸ë“¤ì˜ OOF ì˜ˆì¸¡ ìƒì„±
        tscv = TimeSeriesSplit(n_splits=n_folds)
        
        oof_lgbm = np.zeros(len(X_train))
        oof_xgb = np.zeros(len(X_train))
        oof_cat = np.zeros(len(X_train))
        
        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):
            X_tr, X_vl = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_vl = y_train.iloc[train_idx], y_train.iloc[val_idx]
            
            # LightGBM
            lgbm = lgb.LGBMRegressor(
                n_estimators=200, learning_rate=0.05, max_depth=8,
                num_leaves=64, subsample=0.8, colsample_bytree=0.8,
                verbose=-1, random_state=42
            )
            lgbm.fit(X_tr, y_tr)
            oof_lgbm[val_idx] = lgbm.predict(X_vl)
            
            # XGBoost
            xgb_model = xgb.XGBRegressor(
                n_estimators=200, learning_rate=0.05, max_depth=8,
                subsample=0.8, colsample_bytree=0.8,
                verbosity=0, random_state=42
            )
            xgb_model.fit(X_tr, y_tr)
            oof_xgb[val_idx] = xgb_model.predict(X_vl)
            
            # CatBoost
            cat = CatBoostRegressor(
                iterations=200, learning_rate=0.05, depth=8,
                verbose=False, random_seed=42
            )
            cat.fit(X_tr, y_tr)
            oof_cat[val_idx] = cat.predict(X_vl)
        
        # Level 2: ë©”íƒ€ ëª¨ë¸ í•™ìŠµ
        if verbose:
            print(f"    [Level 2] ë©”íƒ€ ëª¨ë¸ í•™ìŠµ...")
        
        meta_features = np.column_stack([oof_lgbm, oof_xgb, oof_cat])
        
        # 0ì´ ì•„ë‹Œ ë¶€ë¶„ë§Œ ì‚¬ìš© (ì²« ë²ˆì§¸ fold ì´ì „ì€ OOFê°€ ì—†ìŒ)
        valid_mask = (oof_lgbm != 0) | (oof_xgb != 0) | (oof_cat != 0)
        
        self.meta_model = Ridge(alpha=1.0)
        self.meta_model.fit(meta_features[valid_mask], y_train.values[valid_mask])
        
        if verbose:
            weights = self.meta_model.coef_
            print(f"      - ë©”íƒ€ ê°€ì¤‘ì¹˜: LGBM={weights[0]:.3f}, XGB={weights[1]:.3f}, CAT={weights[2]:.3f}")
        
        # ì „ì²´ ë°ì´í„°ë¡œ ê¸°ë³¸ ëª¨ë¸ ì¬í•™ìŠµ
        if verbose:
            print(f"    [Retrain] ì „ì²´ ë°ì´í„°ë¡œ ê¸°ë³¸ ëª¨ë¸ ì¬í•™ìŠµ...")
        
        self.base_models['lgbm'] = lgb.LGBMRegressor(
            n_estimators=300, learning_rate=0.05, max_depth=8,
            num_leaves=64, subsample=0.8, colsample_bytree=0.8,
            verbose=-1, random_state=42
        )
        self.base_models['lgbm'].fit(X_train, y_train)
        
        self.base_models['xgb'] = xgb.XGBRegressor(
            n_estimators=300, learning_rate=0.05, max_depth=8,
            subsample=0.8, colsample_bytree=0.8,
            verbosity=0, random_state=42
        )
        self.base_models['xgb'].fit(X_train, y_train)
        
        self.base_models['cat'] = CatBoostRegressor(
            iterations=300, learning_rate=0.05, depth=8,
            verbose=False, random_seed=42
        )
        self.base_models['cat'].fit(X_train, y_train)
        
        return self
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """Stacking ì˜ˆì¸¡"""
        pred_lgbm = self.base_models['lgbm'].predict(X)
        pred_xgb = self.base_models['xgb'].predict(X)
        pred_cat = self.base_models['cat'].predict(X)
        
        meta_features = np.column_stack([pred_lgbm, pred_xgb, pred_cat])
        
        return self.meta_model.predict(meta_features)


# ============================================================================
# 3. ì‹œê³„ì—´ êµì°¨ê²€ì¦ (Expanding Window)
# ============================================================================

class ExpandingWindowCV:
    """
    Expanding Window Cross-Validation:
    - ì‹œê°„ìˆœìœ¼ë¡œ ì ì  ì»¤ì§€ëŠ” í•™ìŠµ ë°ì´í„°
    - ë¯¸ë˜ ë°ì´í„° ëˆ„ì¶œ ë°©ì§€
    """
    
    def __init__(self, initial_train_months: int = 36, test_months: int = 3, step: int = 3):
        self.initial_train_months = initial_train_months
        self.test_months = test_months
        self.step = step
        
    def split(self, df: pd.DataFrame, time_col: str = 'month'):
        """Expanding Window ë¶„í•  ìƒì„±"""
        months = sorted(df[time_col].unique())
        n_months = len(months)
        
        folds = []
        start = self.initial_train_months
        
        while start + self.test_months <= n_months:
            train_months = months[:start]
            test_months = months[start:start + self.test_months]
            
            train_idx = df[df[time_col].isin(train_months)].index.tolist()
            test_idx = df[df[time_col].isin(test_months)].index.tolist()
            
            folds.append((train_idx, test_idx))
            start += self.step
        
        return folds
    
    def evaluate_model(self, model_class, df: pd.DataFrame, 
                      feature_cols: list, target_col: str,
                      time_col: str = 'month', verbose: bool = True):
        """Expanding Window CVë¡œ ëª¨ë¸ í‰ê°€"""
        
        folds = self.split(df, time_col)
        
        all_metrics = []
        
        for fold_idx, (train_idx, test_idx) in enumerate(folds):
            X_train = df.loc[train_idx, feature_cols]
            y_train = df.loc[train_idx, target_col]
            X_test = df.loc[test_idx, feature_cols]
            y_test = df.loc[test_idx, target_col]
            
            # ëª¨ë¸ í•™ìŠµ
            model = model_class()
            model.fit(X_train, y_train)
            
            # ì˜ˆì¸¡ ë° í‰ê°€
            y_pred = model.predict(X_test)
            
            mae = mean_absolute_error(y_test, y_pred)
            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            r2 = r2_score(y_test, y_pred)
            
            # SMAPE
            denom = (np.abs(y_test) + np.abs(y_pred)) / 2.0
            denom = np.maximum(denom, 1e-9)
            smape = np.mean(np.abs(y_test - y_pred) / denom)
            
            all_metrics.append({
                'fold': fold_idx + 1,
                'train_size': len(train_idx),
                'test_size': len(test_idx),
                'mae': mae,
                'rmse': rmse,
                'r2': r2,
                'smape': smape
            })
            
            if verbose:
                print(f"      Fold {fold_idx+1}: Train={len(train_idx):,}, Test={len(test_idx):,}, "
                      f"RÂ²={r2:.4f}, SMAPE={smape:.4f}")
        
        return pd.DataFrame(all_metrics)


# ============================================================================
# ë©”íŠ¸ë¦­ í•¨ìˆ˜
# ============================================================================

def compute_metrics(y_true, y_pred):
    """ëª¨ë“  ë©”íŠ¸ë¦­ ê³„ì‚°"""
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    
    # SMAPE
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denom = np.maximum(denom, 1e-9)
    smape = np.mean(np.abs(y_true - y_pred) / denom)
    
    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'smape': smape}


# ============================================================================
# ë©”ì¸ í•¨ìˆ˜
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="ê³ ê¸‰ ëª¨ë¸ ê°œì„ ")
    parser.add_argument("--raw_csv", type=str, default="../ì™¸ë¶€ ë°ì´í„°/iMbank_data.csv.csv")
    parser.add_argument("--external_dir", type=str, default="../ì™¸ë¶€ ë°ì´í„°")
    parser.add_argument("--output_dir", type=str, default="outputs/advanced_models")
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("=" * 60)
    print("ğŸš€ ê³ ê¸‰ ëª¨ë¸ ê°œì„  ì‹œì‘")
    print("=" * 60)
    
    # ========================================================================
    # ë°ì´í„° ë¡œë“œ
    # ========================================================================
    print("\n[1/4] ë°ì´í„° ë¡œë“œ...")
    
    cfg = RadarConfig()
    raw_df = load_internal_csv(Path(args.raw_csv), cfg)
    print(f"  âœ“ ì›ë³¸ ë°ì´í„°: {len(raw_df):,}í–‰")
    
    print("[2/4] ì „ì²˜ë¦¬...")
    monthly_df = aggregate_to_segment_month(raw_df, cfg)
    panel_result = build_full_panel(monthly_df, cfg)
    panel_df = panel_result.panel if hasattr(panel_result, 'panel') else panel_result
    print(f"  âœ“ íŒ¨ë„ ë°ì´í„°: {len(panel_df):,}í–‰")
    
    print("[3/4] KPI ë° í”¼ì²˜ ìƒì„±...")
    kpi_df = compute_kpis(panel_df, cfg)
    
    # ì™¸ë¶€ ë°ì´í„° ë³‘í•©
    external_dir = Path(args.external_dir)
    external_dict = load_all_external_data(external_dir)
    kpi_df = merge_external_to_panel(kpi_df, external_dict)
    kpi_df = add_macro_lag_features(kpi_df, lags=[1, 2, 3])
    
    # Lag í”¼ì²˜ - value_cols ì •ì˜
    target_cols_list = ['log1p_ì˜ˆê¸ˆì´ì”ì•¡', 'log1p_ëŒ€ì¶œì´ì”ì•¡', 'log1p_ì¹´ë“œì´ì‚¬ìš©', 
                       'log1p_ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡', 'slog1p_ìˆœìœ ì…', 'log1p_FXì´ì•¡']
    value_cols = [c for c in target_cols_list if c in kpi_df.columns]
    value_cols += [c for c in ['í•œë„ì†Œì§„ìœ¨', 'ë””ì§€í„¸ë¹„ì¤‘', 'ìë™ì´ì²´ë¹„ì¤‘', 'customer_count'] if c in kpi_df.columns]
    
    kpi_df = add_lag_features(kpi_df, group_col="segment_id", time_col="month", value_cols=value_cols)
    kpi_df = add_rolling_features(kpi_df, group_col="segment_id", time_col="month", value_cols=value_cols)
    kpi_df = add_change_features(kpi_df, group_col="segment_id", time_col="month", value_cols=value_cols)
    print(f"  âœ“ ìµœì¢… ë°ì´í„°: {len(kpi_df):,}í–‰, {len(kpi_df.columns)}ê°œ ì»¬ëŸ¼")
    
    # ========================================================================
    # í”¼ì²˜ ë° íƒ€ê²Ÿ ì¤€ë¹„
    # ========================================================================
    print("[4/4] í”¼ì²˜ ì¤€ë¹„...")
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼
    target_cols = {
        'ì˜ˆê¸ˆì´ì”ì•¡': 'log1p_ì˜ˆê¸ˆì´ì”ì•¡',
        'ëŒ€ì¶œì´ì”ì•¡': 'log1p_ëŒ€ì¶œì´ì”ì•¡', 
        'ì¹´ë“œì´ì‚¬ìš©': 'log1p_ì¹´ë“œì´ì‚¬ìš©',
        'ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡': 'log1p_ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡',
        'ìˆœìœ ì…': 'slog1p_ìˆœìœ ì…',
        'FXì´ì•¡': 'log1p_FXì´ì•¡'
    }
    
    # Horizon íƒ€ê²Ÿ ìƒì„±
    for kpi, base_col in target_cols.items():
        for h in [1, 2, 3]:
            target_name = f"target_{kpi}_h{h}"
            kpi_df[target_name] = kpi_df.groupby('segment_id')[base_col].shift(-h)
    
    # í”¼ì²˜ ì»¬ëŸ¼ ì„ íƒ
    exclude_cols = ['segment_id', 'month', 'age_group', 'job', 'gender', 'region',
                   'segment_name', 'active_rate']
    exclude_cols += [c for c in kpi_df.columns if c.startswith('target_')]
    exclude_cols += [c for c in kpi_df.columns if 'log1p_' in c or 'slog1p_' in c]
    
    feature_cols = [c for c in kpi_df.columns if c not in exclude_cols 
                   and kpi_df[c].dtype in ['float64', 'int64', 'float32', 'int32']]
    
    categorical_cols = ['age_group', 'job', 'gender', 'region']
    
    print(f"  âœ“ í”¼ì²˜: {len(feature_cols)}ê°œ")
    
    # Train/Test ë¶„í• 
    train_end = pd.Timestamp("2024-09-01")
    train_mask = kpi_df['month'] <= train_end
    
    # NaN ì œê±°
    valid_mask = kpi_df[feature_cols].notna().all(axis=1)
    
    results = []
    
    # ========================================================================
    # 1. ìˆœìœ ì… Two-Stage ëª¨ë¸
    # ========================================================================
    print("\n" + "=" * 60)
    print("ğŸ“Š 1. ìˆœìœ ì… Two-Stage ëª¨ë¸")
    print("=" * 60)
    
    for horizon in [1, 2, 3]:
        print(f"\n[ìˆœìœ ì…] Horizon {horizon}ê°œì›”")
        print("-" * 40)
        
        target_col = f"target_ìˆœìœ ì…_h{horizon}"
        
        # ë°ì´í„° ì¤€ë¹„ - ë” ë‚˜ì€ ë¶„í• 
        mask = train_mask & valid_mask & kpi_df[target_col].notna()
        test_mask = (~train_mask) & valid_mask & kpi_df[target_col].notna()
        
        X_train = kpi_df.loc[mask, feature_cols].copy()
        y_train = kpi_df.loc[mask, target_col].copy()
        X_test = kpi_df.loc[test_mask, feature_cols].copy()
        y_test = kpi_df.loc[test_mask, target_col].copy()
        
        print(f"  Train: {len(X_train):,}, Test: {len(X_test):,}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
        if len(X_test) < 10:
            print(f"  âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡±, ìŠ¤í‚µ")
            continue
        
        # Two-Stage ëª¨ë¸ í•™ìŠµ
        two_stage = TwoStageNetFlowModel(horizon=horizon)
        two_stage.fit(X_train, y_train, categorical_cols=categorical_cols)
        
        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred_twostage = two_stage.predict(X_test)
        metrics_twostage = compute_metrics(y_test.values, y_pred_twostage)
        
        print(f"\n  âœ… Two-Stage ê²°ê³¼:")
        print(f"     RÂ² = {metrics_twostage['r2']:.4f}")
        print(f"     SMAPE = {metrics_twostage['smape']:.4f}")
        
        # ê¸°ì¡´ ë‹¨ì¼ ëª¨ë¸ ë¹„êµ
        baseline = lgb.LGBMRegressor(
            n_estimators=300, learning_rate=0.05, max_depth=8,
            verbose=-1, random_state=42
        )
        baseline.fit(X_train, y_train)
        y_pred_baseline = baseline.predict(X_test)
        metrics_baseline = compute_metrics(y_test.values, y_pred_baseline)
        
        print(f"\n  ğŸ“Š ê¸°ì¡´ ëª¨ë¸:")
        print(f"     RÂ² = {metrics_baseline['r2']:.4f}")
        print(f"     SMAPE = {metrics_baseline['smape']:.4f}")
        
        improvement = (metrics_twostage['r2'] - metrics_baseline['r2']) / max(abs(metrics_baseline['r2']), 0.001) * 100
        print(f"\n  ğŸ¯ RÂ² ê°œì„ : {improvement:+.1f}%")
        
        results.append({
            'kpi': 'ìˆœìœ ì…',
            'horizon': horizon,
            'model': 'Two-Stage',
            'r2': metrics_twostage['r2'],
            'smape': metrics_twostage['smape'],
            'baseline_r2': metrics_baseline['r2'],
            'improvement': improvement
        })
    
    # ========================================================================
    # 2. Stacking ì•™ìƒë¸” (ì£¼ìš” KPI)
    # ========================================================================
    print("\n" + "=" * 60)
    print("ğŸ“Š 2. Stacking ì•™ìƒë¸”")
    print("=" * 60)
    
    for kpi in ['ì˜ˆê¸ˆì´ì”ì•¡', 'ëŒ€ì¶œì´ì”ì•¡', 'ìˆœìœ ì…']:
        for horizon in [1]:  # 1ê°œì›”ë§Œ í…ŒìŠ¤íŠ¸
            print(f"\n[{kpi}] Horizon {horizon}ê°œì›”")
            print("-" * 40)
            
            target_col = f"target_{kpi}_h{horizon}"
            
            # ë°ì´í„° ì¤€ë¹„
            mask = train_mask & valid_mask & kpi_df[target_col].notna()
            test_mask = (~train_mask) & valid_mask & kpi_df[target_col].notna()
            
            X_train = kpi_df.loc[mask, feature_cols].copy()
            y_train = kpi_df.loc[mask, target_col].copy()
            X_test = kpi_df.loc[test_mask, feature_cols].copy()
            y_test = kpi_df.loc[test_mask, target_col].copy()
            
            print(f"  Train: {len(X_train):,}, Test: {len(X_test):,}")
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìŠ¤í‚µ
            if len(X_test) < 10:
                print(f"  âš ï¸ í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶€ì¡±, ìŠ¤í‚µ")
                continue
            
            # Stacking í•™ìŠµ
            stacking = StackingEnsemble()
            stacking.fit(X_train, y_train, categorical_cols=categorical_cols, n_folds=3)
            
            # ì˜ˆì¸¡ ë° í‰ê°€
            y_pred_stack = stacking.predict(X_test)
            metrics_stack = compute_metrics(y_test.values, y_pred_stack)
            
            print(f"\n  âœ… Stacking ê²°ê³¼:")
            print(f"     RÂ² = {metrics_stack['r2']:.4f}")
            print(f"     SMAPE = {metrics_stack['smape']:.4f}")
            
            # ê¸°ì¡´ ë‹¨ì¼ ëª¨ë¸ ë¹„êµ
            baseline = lgb.LGBMRegressor(
                n_estimators=300, learning_rate=0.05, max_depth=8,
                verbose=-1, random_state=42
            )
            baseline.fit(X_train, y_train)
            y_pred_baseline = baseline.predict(X_test)
            metrics_baseline = compute_metrics(y_test.values, y_pred_baseline)
            
            improvement = (metrics_stack['r2'] - metrics_baseline['r2']) / max(abs(metrics_baseline['r2']), 0.001) * 100
            print(f"\n  ğŸ¯ RÂ² ê°œì„ : {improvement:+.1f}%")
            
            results.append({
                'kpi': kpi,
                'horizon': horizon,
                'model': 'Stacking',
                'r2': metrics_stack['r2'],
                'smape': metrics_stack['smape'],
                'baseline_r2': metrics_baseline['r2'],
                'improvement': improvement
            })
    
    # ========================================================================
    # 3. ì‹œê³„ì—´ CV í‰ê°€
    # ========================================================================
    print("\n" + "=" * 60)
    print("ğŸ“Š 3. ì‹œê³„ì—´ êµì°¨ê²€ì¦ (Expanding Window)")
    print("=" * 60)
    
    # ìˆœìœ ì…ì— ëŒ€í•´ Expanding Window CV ìˆ˜í–‰
    target_col = "target_ìˆœìœ ì…_h1"
    mask = valid_mask & kpi_df[target_col].notna()
    eval_df = kpi_df.loc[mask].copy()
    
    print(f"\n[ìˆœìœ ì…] Expanding Window CV")
    print("-" * 40)
    
    cv = ExpandingWindowCV(initial_train_months=36, test_months=3, step=3)
    
    class LGBMWrapper:
        def __init__(self):
            self.model = None
        def fit(self, X, y):
            self.model = lgb.LGBMRegressor(
                n_estimators=200, learning_rate=0.05, max_depth=8,
                verbose=-1, random_state=42
            )
            self.model.fit(X, y)
        def predict(self, X):
            return self.model.predict(X)
    
    cv_results = cv.evaluate_model(
        LGBMWrapper, eval_df, feature_cols, target_col, 
        time_col='month', verbose=True
    )
    
    print(f"\n  ğŸ“Š CV í‰ê·  ê²°ê³¼:")
    if len(cv_results) > 0 and 'r2' in cv_results.columns:
        print(f"     í‰ê·  RÂ² = {cv_results['r2'].mean():.4f} (Â±{cv_results['r2'].std():.4f})")
        print(f"     í‰ê·  SMAPE = {cv_results['smape'].mean():.4f} (Â±{cv_results['smape'].std():.4f})")
    else:
        print(f"     âš ï¸ CV ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤ (ë°ì´í„° ë¶€ì¡± ë˜ëŠ” fold ìƒì„± ì‹¤íŒ¨)")
        cv_results = pd.DataFrame(columns=['fold', 'train_size', 'test_size', 'mae', 'rmse', 'r2', 'smape'])
    
    # ========================================================================
    # ê²°ê³¼ ì €ì¥
    # ========================================================================
    results_df = pd.DataFrame(results)
    results_df.to_csv(output_dir / "advanced_model_results.csv", index=False)
    cv_results.to_csv(output_dir / "cv_results.csv", index=False)
    
    print("\n" + "=" * 60)
    print("âœ… ê³ ê¸‰ ëª¨ë¸ ê°œì„  ì™„ë£Œ!")
    print("=" * 60)
    
    print("\nğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½:")
    print(results_df.to_string(index=False))
    
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: {output_dir}")


if __name__ == "__main__":
    main()
