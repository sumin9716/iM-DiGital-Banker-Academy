"""
ì›ì¸ ë¶„ì„ ì—”ì§„ - KPI ë³€ë™ ì›ì¸ì„ ìì—°ì–´ë¡œ ìš”ì•½

Features:
- í”¼ì²˜ ê¸°ì—¬ë„ ê¸°ë°˜ ì›ì¸ ë¶„ì„
- ê±°ì‹œê²½ì œ ìš”ì¸ ì—°ë™
- ë¹„ì¦ˆë‹ˆìŠ¤ ë§¥ë½ ë°˜ì˜
- ìì—°ì–´ ì„¤ëª… ìƒì„±
"""
from __future__ import annotations

from dataclasses import dataclass
from typing import List, Dict, Optional, Tuple
import pandas as pd
import numpy as np


# í”¼ì²˜ëª… â†’ ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ ë§¤í•‘
FEATURE_DISPLAY_NAMES = {
    # KPI ê´€ë ¨
    "ì˜ˆê¸ˆì´ì”ì•¡": "ì˜ˆê¸ˆì”ì•¡",
    "ëŒ€ì¶œì´ì”ì•¡": "ëŒ€ì¶œì”ì•¡",
    "ìˆœìœ ì…": "ìˆœìœ ì…",
    "ì¹´ë“œì´ì‚¬ìš©": "ì¹´ë“œì‚¬ìš©",
    "ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡": "ë””ì§€í„¸ê±°ë˜",
    "FXì´ì•¡": "ì™¸í™˜ê±°ë˜",
    
    # Lag í”¼ì²˜
    "lag1": "ì „ì›”",
    "lag2": "2ê°œì›”ì „",
    "lag3": "3ê°œì›”ì „",
    "lag6": "6ê°œì›”ì „",
    "lag12": "ì „ë…„ë™ì›”",
    
    # Rolling í”¼ì²˜
    "roll3_mean": "3ê°œì›”í‰ê· ",
    "roll6_mean": "6ê°œì›”í‰ê· ",
    "roll12_mean": "ì—°í‰ê· ",
    
    # ë³€í™”ìœ¨
    "pct_chg": "ë³€í™”ìœ¨",
    "mom": "ì „ì›”ë¹„",
    "yoy": "ì „ë…„ë¹„",
    
    # ê±°ì‹œê²½ì œ
    "fed_rate": "ë¯¸êµ­ê¸°ì¤€ê¸ˆë¦¬",
    "fed_diff": "ê¸ˆë¦¬ì°¨",
    "usd_krw": "ë‹¬ëŸ¬í™˜ìœ¨",
    "usd_krw_vol": "í™˜ìœ¨ë³€ë™ì„±",
    "brent": "ìœ ê°€",
    "esi": "ê²½ê¸°ì „ë§ì§€ìˆ˜",
    "kasi": "í•œêµ­ì‹ ë¢°ì§€ìˆ˜",
    "apt_price": "ì•„íŒŒíŠ¸ê°€ê²©",
    "apt_vol": "ë¶€ë™ì‚°ê±°ë˜ëŸ‰",
    "jeonse": "ì „ì„¸ì§€ìˆ˜",
    "trade_balance": "ë¬´ì—­ìˆ˜ì§€",
    
    # ì„¸ê·¸ë¨¼íŠ¸
    "ì—°ë ¹ëŒ€": "ì—°ë ¹ëŒ€",
    "ì§ì—…êµ°": "ì§ì—…êµ°",
    "ê±°ë˜í™œì„±ë„": "í™œì„±ë„",
    "ìì‚°ê·œëª¨": "ìì‚°ë“±ê¸‰",
}


@dataclass
class CauseAnalysis:
    """ì›ì¸ ë¶„ì„ ê²°ê³¼"""
    kpi: str
    direction: str  # "ì¦ê°€", "ê°ì†Œ", "ê¸‰ë“±", "ê¸‰ë½"
    magnitude: float  # ë³€í™” í¬ê¸° (%)
    primary_causes: List[str]  # ì£¼ìš” ì›ì¸
    secondary_causes: List[str]  # ë¶€ê°€ ì›ì¸
    macro_factors: List[str]  # ê±°ì‹œê²½ì œ ìš”ì¸
    summary: str  # ì¢…í•© ìš”ì•½
    confidence: float  # ë¶„ì„ ì‹ ë¢°ë„


def format_feature_name(feature: str) -> str:
    """í”¼ì²˜ëª…ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ë¡œ ë³€í™˜"""
    result = feature
    
    # log1p ì ‘ë‘ì‚¬ ì œê±°
    result = result.replace("log1p_", "")
    
    # ì–¸ë”ìŠ¤ì½”ì–´ ì²˜ë¦¬
    parts = result.split("_")
    formatted_parts = []
    
    for part in parts:
        # ë§¤í•‘ëœ ì´ë¦„ ì‚¬ìš©
        if part in FEATURE_DISPLAY_NAMES:
            formatted_parts.append(FEATURE_DISPLAY_NAMES[part])
        elif part.isdigit():
            continue  # ìˆ«ìëŠ” ê±´ë„ˆëœ€
        else:
            formatted_parts.append(part)
    
    return " ".join(formatted_parts).strip()


def interpret_contribution(
    feature: str,
    contribution: float,
    feature_value: Optional[float] = None,
) -> str:
    """
    í”¼ì²˜ ê¸°ì—¬ë„ë¥¼ ìì—°ì–´ë¡œ í•´ì„
    
    Args:
        feature: í”¼ì²˜ëª…
        contribution: ê¸°ì—¬ë„ ê°’
        feature_value: í”¼ì²˜ ì‹¤ì œê°’ (ì„ íƒ)
        
    Returns:
        í•´ì„ ë¬¸ì¥
    """
    display_name = format_feature_name(feature)
    direction = "ìƒìŠ¹" if contribution > 0 else "í•˜ë½"
    strength = abs(contribution)
    
    # ê°•ë„ í‘œí˜„
    if strength > 0.5:
        intensity = "í¬ê²Œ"
    elif strength > 0.2:
        intensity = "ìƒë‹¹íˆ"
    elif strength > 0.1:
        intensity = ""
    else:
        intensity = "ì†Œí­"
    
    # Lag/Rolling í”¼ì²˜ í•´ì„
    if "lag" in feature.lower() or "roll" in feature.lower():
        if "lag1" in feature:
            time_ref = "ì „ì›” ì¶”ì„¸"
        elif "lag3" in feature or "roll3" in feature:
            time_ref = "ìµœê·¼ 3ê°œì›” ì¶”ì„¸"
        elif "lag6" in feature or "roll6" in feature:
            time_ref = "ì¤‘ê¸° ì¶”ì„¸"
        elif "lag12" in feature:
            time_ref = "ì „ë…„ ë™ê¸° ëŒ€ë¹„"
        else:
            time_ref = "ê³¼ê±° ì¶”ì„¸"
        
        return f"{display_name}ì˜ {time_ref}ê°€ {direction} ë°©í–¥ìœ¼ë¡œ {intensity} ê¸°ì—¬"
    
    # ê±°ì‹œê²½ì œ í”¼ì²˜ í•´ì„
    macro_keywords = ["fed", "usd", "krw", "brent", "esi", "kasi", "apt", "jeonse", "trade"]
    if any(kw in feature.lower() for kw in macro_keywords):
        return f"ê±°ì‹œìš”ì¸({display_name}) {direction} ì˜í–¥"
    
    # ë³€í™”ìœ¨ í”¼ì²˜
    if "pct_chg" in feature or "mom" in feature or "yoy" in feature:
        return f"{display_name} ë³€í™”ìœ¨ì´ {direction} ê¸°ì—¬"
    
    # ì¼ë°˜ í”¼ì²˜
    return f"{display_name}ì´(ê°€) {intensity} {direction} ì˜í–¥"


def analyze_cause(
    kpi: str,
    residual_pct: float,
    feature_contributions: Dict[str, float],
    segment_data: Optional[pd.Series] = None,
    macro_data: Optional[Dict] = None,
    top_k: int = 5,
) -> CauseAnalysis:
    """
    KPI ë³€ë™ ì›ì¸ ë¶„ì„
    
    Args:
        kpi: KPIëª…
        residual_pct: ì”ì°¨ ë¹„ìœ¨ (ì˜ˆì¸¡ ëŒ€ë¹„ ì‹¤ì œ ì°¨ì´)
        feature_contributions: í”¼ì²˜ë³„ ê¸°ì—¬ë„
        segment_data: ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„°
        macro_data: ê±°ì‹œê²½ì œ ë°ì´í„°
        top_k: ìƒìœ„ ì›ì¸ ê°œìˆ˜
        
    Returns:
        CauseAnalysis ê²°ê³¼
    """
    # ë°©í–¥ ë° í¬ê¸° ê²°ì •
    magnitude = abs(residual_pct * 100)
    if residual_pct > 0.2:
        direction = "ê¸‰ë“±"
    elif residual_pct > 0.05:
        direction = "ì¦ê°€"
    elif residual_pct < -0.2:
        direction = "ê¸‰ë½"
    elif residual_pct < -0.05:
        direction = "ê°ì†Œ"
    else:
        direction = "ë³€ë™"
    
    # ê¸°ì—¬ë„ ì •ë ¬
    sorted_contribs = sorted(
        feature_contributions.items(),
        key=lambda x: abs(x[1]),
        reverse=True
    )
    
    # ì£¼ìš” ì›ì¸ (ë™ì¼ ë°©í–¥ ê¸°ì—¬)
    if residual_pct > 0:
        primary_contribs = [(f, v) for f, v in sorted_contribs if v > 0][:top_k]
    else:
        primary_contribs = [(f, v) for f, v in sorted_contribs if v < 0][:top_k]
    
    primary_causes = [
        interpret_contribution(f, v) 
        for f, v in primary_contribs
    ]
    
    # ë¶€ê°€ ì›ì¸ (ë°˜ëŒ€ ë°©í–¥ ê¸°ì—¬)
    if residual_pct > 0:
        secondary_contribs = [(f, v) for f, v in sorted_contribs if v < 0][:3]
    else:
        secondary_contribs = [(f, v) for f, v in sorted_contribs if v > 0][:3]
    
    secondary_causes = [
        interpret_contribution(f, v)
        for f, v in secondary_contribs
    ]
    
    # ê±°ì‹œê²½ì œ ìš”ì¸ ë¶„ë¦¬
    macro_keywords = ["fed", "usd", "krw", "brent", "esi", "kasi", "apt", "jeonse", "trade"]
    macro_factors = []
    for feature, contrib in sorted_contribs[:10]:
        if any(kw in feature.lower() for kw in macro_keywords):
            macro_factors.append(interpret_contribution(feature, contrib))
    macro_factors = macro_factors[:3]
    
    # ì¢…í•© ìš”ì•½ ìƒì„±
    summary = generate_cause_summary(
        kpi=kpi,
        direction=direction,
        magnitude=magnitude,
        primary_causes=primary_causes,
        macro_factors=macro_factors,
        segment_data=segment_data,
    )
    
    # ì‹ ë¢°ë„ (ê¸°ì—¬ë„ ì§‘ì¤‘ë„ ê¸°ë°˜)
    if sorted_contribs:
        top_contrib_sum = sum(abs(v) for _, v in sorted_contribs[:3])
        total_contrib = sum(abs(v) for _, v in sorted_contribs)
        confidence = min(top_contrib_sum / (total_contrib + 1e-9), 1.0)
    else:
        confidence = 0.5
    
    return CauseAnalysis(
        kpi=kpi,
        direction=direction,
        magnitude=magnitude,
        primary_causes=primary_causes,
        secondary_causes=secondary_causes,
        macro_factors=macro_factors,
        summary=summary,
        confidence=confidence,
    )


def generate_cause_summary(
    kpi: str,
    direction: str,
    magnitude: float,
    primary_causes: List[str],
    macro_factors: List[str],
    segment_data: Optional[pd.Series] = None,
) -> str:
    """
    ì›ì¸ ë¶„ì„ ì¢…í•© ìš”ì•½ ìƒì„±
    
    Args:
        kpi: KPIëª…
        direction: ë³€ë™ ë°©í–¥
        magnitude: ë³€ë™ í¬ê¸°
        primary_causes: ì£¼ìš” ì›ì¸
        macro_factors: ê±°ì‹œê²½ì œ ìš”ì¸
        segment_data: ì„¸ê·¸ë¨¼íŠ¸ ë°ì´í„°
        
    Returns:
        ì¢…í•© ìš”ì•½ ë¬¸ì¥
    """
    parts = []
    
    # ë„ì…ë¶€
    parts.append(f"{kpi}ì´(ê°€) ì˜ˆì¸¡ ëŒ€ë¹„ {magnitude:.1f}% {direction}í•˜ì˜€ìŠµë‹ˆë‹¤.")
    
    # ì£¼ìš” ì›ì¸
    if primary_causes:
        if len(primary_causes) == 1:
            parts.append(f"ì£¼ìš” ì›ì¸: {primary_causes[0]}.")
        else:
            top_causes = ", ".join(primary_causes[:2])
            parts.append(f"ì£¼ìš” ì›ì¸: {top_causes}.")
    
    # ê±°ì‹œê²½ì œ ìš”ì¸
    if macro_factors:
        parts.append(f"ì™¸ë¶€ ìš”ì¸: {macro_factors[0]}.")
    
    # ì„¸ê·¸ë¨¼íŠ¸ ë§¥ë½
    if segment_data is not None:
        age_group = segment_data.get("ì—°ë ¹ëŒ€", "")
        job_group = segment_data.get("ì§ì—…êµ°", "")
        activity = segment_data.get("ê±°ë˜í™œì„±ë„", "")
        
        context_parts = []
        if age_group:
            context_parts.append(age_group)
        if job_group:
            context_parts.append(job_group)
        if activity:
            context_parts.append(f"{activity} ê³ ê°")
        
        if context_parts:
            segment_desc = " ".join(context_parts)
            
            # ì„¸ê·¸ë¨¼íŠ¸ ë§¥ë½ í•´ì„
            if direction in ["ê¸‰ë½", "ê°ì†Œ"]:
                if activity == "ì €í™œì„±":
                    parts.append(f"({segment_desc}: ê´€ê³„ ì•½í™” ì§•í›„ ì£¼ì˜)")
                elif "60ëŒ€" in age_group or "70ëŒ€" in age_group:
                    parts.append(f"({segment_desc}: ì‹œë‹ˆì–´ ìê¸ˆì´ë™ ê°€ëŠ¥ì„±)")
                elif "ìì˜ì—…" in job_group or "ì†Œìƒê³µì¸" in job_group:
                    parts.append(f"({segment_desc}: ì‚¬ì—…ìê¸ˆ ë³€ë™ ê°€ëŠ¥ì„±)")
            elif direction in ["ê¸‰ë“±", "ì¦ê°€"]:
                if "VIP" in str(segment_data.get("ìì‚°ê·œëª¨", "")):
                    parts.append(f"({segment_desc}: ìì‚°ì´ë™ ë˜ëŠ” ê±°ë˜í™•ëŒ€)")
    
    return " ".join(parts)


def generate_watchlist_driver_summary(
    row: pd.Series,
    feature_contributions: Optional[Dict[str, float]] = None,
) -> str:
    """
    ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ í•­ëª©ì˜ ë“œë¼ì´ë²„ ìš”ì•½
    
    Args:
        row: ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ í–‰
        feature_contributions: í”¼ì²˜ ê¸°ì—¬ë„
        
    Returns:
        ë“œë¼ì´ë²„ ìš”ì•½ ë¬¸ìì—´
    """
    drivers = []
    
    # ì•Œë¦¼ ìœ í˜• ê¸°ë°˜
    alert_type = str(row.get("alert_type", ""))
    severity = str(row.get("severity", ""))
    residual_pct = float(row.get("residual_pct", 0) or 0)
    
    if alert_type == "DROP":
        drivers.append(f"ì˜ˆì¸¡ ëŒ€ë¹„ {abs(residual_pct)*100:.1f}% í•˜ë½")
    elif alert_type == "SPIKE":
        drivers.append(f"ì˜ˆì¸¡ ëŒ€ë¹„ {abs(residual_pct)*100:.1f}% ìƒìŠ¹")
    elif alert_type == "INFLECTION":
        drivers.append("ì¶”ì„¸ ì „í™˜ì  ê°ì§€")
    
    # ì‹¬ê°ë„ ê¸°ë°˜
    if severity == "CRITICAL":
        drivers.append("ìœ„í—˜ ìˆ˜ì¤€ ì‹¬ê°")
    elif severity == "HIGH":
        drivers.append("ì£¼ì˜ í•„ìš”")
    
    # í”¼ì²˜ ê¸°ì—¬ë„ ê¸°ë°˜
    if feature_contributions:
        sorted_contribs = sorted(
            feature_contributions.items(),
            key=lambda x: abs(x[1]),
            reverse=True
        )[:3]
        
        for feature, contrib in sorted_contribs:
            drivers.append(interpret_contribution(feature, contrib))
    
    # ê¸°íƒ€ ì»¬ëŸ¼ ê¸°ë°˜ ì •ë³´
    risk_factors = str(row.get("risk_factors", ""))
    if risk_factors and risk_factors != "nan":
        factors = risk_factors.split(";")[:2]
        drivers.extend([f.strip() for f in factors if f.strip()])
    
    if not drivers:
        return "ìƒì„¸ ë¶„ì„ í•„ìš”"
    
    return " | ".join(drivers[:4])


def generate_risk_factor_summary(row: pd.Series) -> str:
    """
    ë¦¬ìŠ¤í¬ ìš”ì¸ ìš”ì•½ ìƒì„±
    
    Args:
        row: ë°ì´í„° í–‰
        
    Returns:
        ë¦¬ìŠ¤í¬ ìš”ì¸ ìš”ì•½ ë¬¸ìì—´
    """
    factors = []
    
    # ë¦¬ìŠ¤í¬ ì ìˆ˜
    risk_score = float(row.get("risk_score", 0) or 0)
    if risk_score >= 80:
        factors.append("ğŸ”´ ê³ ìœ„í—˜")
    elif risk_score >= 60:
        factors.append("ğŸŸ  ì¤‘ìœ„í—˜")
    elif risk_score >= 40:
        factors.append("ğŸŸ¡ ê´€ì°°í•„ìš”")
    
    # ìˆœìœ ì… ìƒíƒœ
    net_inflow = float(row.get("ìˆœìœ ì…", 0) or 0)
    if net_inflow < -1000:
        factors.append("ëŒ€ê·œëª¨ ìˆœìœ ì¶œ")
    elif net_inflow < -500:
        factors.append("ìˆœìœ ì¶œ ì¤‘")
    elif net_inflow < 0:
        factors.append("ì†Œí­ ìˆœìœ ì¶œ")
    
    # ì”ì°¨ ìƒíƒœ
    residual_pct = float(row.get("residual_pct", 0) or 0)
    if residual_pct < -0.3:
        factors.append("ì˜ˆì¸¡ ëŒ€ë¹„ ê¸‰ë½")
    elif residual_pct < -0.1:
        factors.append("ì˜ˆì¸¡ í•˜íšŒ")
    
    # ë³€ë™ì„±
    volatility = str(row.get("volatility_regime", ""))
    if "HighVol" in volatility:
        factors.append("í™˜ìœ¨ ë³€ë™ì„±â†‘")
    
    # í™˜ìœ¨ ë ˆì§
    regime = str(row.get("regime", row.get("current_regime", "")))
    if "Uptrend" in regime:
        factors.append("ì›í™”ì•½ì„¸")
    
    if not factors:
        return "íŠ¹ì´ì‚¬í•­ ì—†ìŒ"
    
    return " | ".join(factors)


def enrich_watchlist_with_drivers(
    watchlist: pd.DataFrame,
    contributions_dict: Optional[Dict[str, Dict[str, float]]] = None,
) -> pd.DataFrame:
    """
    ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ì— ë“œë¼ì´ë²„ ì •ë³´ ì¶”ê°€
    
    Args:
        watchlist: ì›Œì¹˜ë¦¬ìŠ¤íŠ¸ DataFrame
        contributions_dict: segment_id â†’ í”¼ì²˜ ê¸°ì—¬ë„ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ë“œë¼ì´ë²„ ì •ë³´ê°€ ì¶”ê°€ëœ DataFrame
    """
    result = watchlist.copy()
    
    # ë“œë¼ì´ë²„ ìš”ì•½ ì»¬ëŸ¼ ì¶”ê°€
    driver_summaries = []
    for _, row in result.iterrows():
        segment_id = str(row.get("segment_id", ""))
        contribs = contributions_dict.get(segment_id) if contributions_dict else None
        summary = generate_watchlist_driver_summary(row, contribs)
        driver_summaries.append(summary)
    
    result["driver_summary"] = driver_summaries
    
    # ë¦¬ìŠ¤í¬ ìš”ì¸ ìš”ì•½
    result["risk_summary"] = result.apply(generate_risk_factor_summary, axis=1)
    
    return result
