"""
ëª¨ë¸ ê°œì„  ìŠ¤í¬ë¦½íŠ¸ v2
===================

ê°œì„  ì‚¬í•­:
1. Optuna ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹
2. LightGBM + XGBoost + CatBoost ì•™ìƒë¸”
3. ìˆœìœ ì…(RÂ²=0.34) íŠ¹ë³„ ê°œì„  ì „ëµ
4. í”¼ì²˜ ì¤‘ìš”ë„ ê¸°ë°˜ ì„ íƒ
5. êµì°¨ê²€ì¦ ê°•í™”

ì‹¤í–‰: python scripts/improve_models_v2.py --raw_csv "../ì™¸ë¶€ ë°ì´í„°/iMbank_data.csv.csv"
"""
from __future__ import annotations

import argparse
import os
import sys
import warnings
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
import lightgbm as lgb
import xgboost as xgb
from catboost import CatBoostRegressor
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

import optuna
from optuna.samplers import TPESampler

warnings.filterwarnings('ignore')
optuna.logging.set_verbosity(optuna.logging.WARNING)

# í”„ë¡œì íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from imradar.config import RadarConfig
from imradar.data.io import load_internal_csv
from imradar.data.preprocess import aggregate_to_segment_month, build_full_panel
from imradar.data.external import load_all_external_data, merge_external_to_panel, add_macro_lag_features
from imradar.features.kpi import compute_kpis
from imradar.features.lags import add_lag_features, add_rolling_features, add_change_features


# ============================================================================
# ë©”íŠ¸ë¦­ í•¨ìˆ˜
# ============================================================================

def smape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-9) -> float:
    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0
    denom = np.maximum(denom, eps)
    return float(np.mean(np.abs(y_true - y_pred) / denom) * 100)


def wmape(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-9) -> float:
    denom = np.sum(np.abs(y_true))
    denom = max(float(denom), eps)
    return float(np.sum(np.abs(y_true - y_pred)) / denom * 100)


def rmse(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    return float(np.sqrt(mean_squared_error(y_true, y_pred)))


# ============================================================================
# Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ============================================================================

def create_lgbm_objective(X_train, y_train, cat_cols):
    """LightGBM Optuna Objective"""
    def objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 300, 1500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
            'num_leaves': trial.suggest_int('num_leaves', 15, 127),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': -1,
        }
        
        # Time Series CV
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        
        for train_idx, val_idx in tscv.split(X_train):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train[train_idx], y_train[val_idx]
            
            model = lgb.LGBMRegressor(**params)
            model.fit(
                X_tr, y_tr,
                eval_set=[(X_val, y_val)],
                callbacks=[lgb.early_stopping(50, verbose=False)],
                categorical_feature=cat_cols,
            )
            
            preds = model.predict(X_val)
            scores.append(smape(y_val, preds))
        
        return np.mean(scores)
    
    return objective


def create_xgb_objective(X_train, y_train):
    """XGBoost Optuna Objective"""
    def objective(trial):
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 300, 1500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'random_state': 42,
            'n_jobs': -1,
            'verbosity': 0,
        }
        
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        
        for train_idx, val_idx in tscv.split(X_train):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train[train_idx], y_train[val_idx]
            
            model = xgb.XGBRegressor(**params, enable_categorical=False)
            model.fit(
                X_tr, y_tr,
                eval_set=[(X_val, y_val)],
                verbose=False,
            )
            
            preds = model.predict(X_val)
            scores.append(smape(y_val, preds))
        
        return np.mean(scores)
    
    return objective


def create_catboost_objective(X_train, y_train, cat_cols):
    """CatBoost Optuna Objective"""
    def objective(trial):
        params = {
            'iterations': trial.suggest_int('iterations', 300, 1500),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.15, log=True),
            'depth': trial.suggest_int('depth', 3, 10),
            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),
            'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),
            'random_seed': 42,
            'verbose': False,
        }
        
        tscv = TimeSeriesSplit(n_splits=3)
        scores = []
        
        for train_idx, val_idx in tscv.split(X_train):
            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]
            y_tr, y_val = y_train[train_idx], y_train[val_idx]
            
            model = CatBoostRegressor(**params)
            model.fit(
                X_tr, y_tr,
                eval_set=[(X_val, y_val)],
                cat_features=cat_cols,
                verbose=False,
                early_stopping_rounds=50,
            )
            
            preds = model.predict(X_val)
            scores.append(smape(y_val, preds))
        
        return np.mean(scores)
    
    return objective


# ============================================================================
# ì•™ìƒë¸” ëª¨ë¸
# ============================================================================

class EnsembleForecaster:
    """LightGBM + XGBoost + CatBoost ì•™ìƒë¸”"""
    
    def __init__(self, kpi: str, horizon: int):
        self.kpi = kpi
        self.horizon = horizon
        self.models = {}
        self.weights = {}
        self.feature_cols = []
        self.cat_cols = []
        self.best_params = {}
    
    def tune_and_train(
        self,
        X_train: pd.DataFrame,
        y_train: np.ndarray,
        cat_cols: list,
        n_trials: int = 30,
        verbose: bool = True,
    ):
        """Optunaë¡œ íŠœë‹ í›„ í•™ìŠµ"""
        self.feature_cols = X_train.columns.tolist()
        self.cat_cols = cat_cols
        
        # ë²”ì£¼í˜• ì¸ì½”ë”© (XGBoostìš©)
        X_train_encoded = X_train.copy()
        for c in cat_cols:
            if c in X_train_encoded.columns:
                X_train_encoded[c] = X_train_encoded[c].astype('category').cat.codes
        
        # 1. LightGBM íŠœë‹
        if verbose:
            print(f"    â””â”€ LightGBM íŠœë‹ ì¤‘ ({n_trials} trials)...")
        study_lgbm = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))
        study_lgbm.optimize(
            create_lgbm_objective(X_train, y_train, cat_cols),
            n_trials=n_trials,
            show_progress_bar=False,
        )
        self.best_params['lgbm'] = study_lgbm.best_params
        
        # 2. XGBoost íŠœë‹
        if verbose:
            print(f"    â””â”€ XGBoost íŠœë‹ ì¤‘ ({n_trials} trials)...")
        study_xgb = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))
        study_xgb.optimize(
            create_xgb_objective(X_train_encoded, y_train),
            n_trials=n_trials,
            show_progress_bar=False,
        )
        self.best_params['xgb'] = study_xgb.best_params
        
        # 3. CatBoost íŠœë‹
        if verbose:
            print(f"    â””â”€ CatBoost íŠœë‹ ì¤‘ ({n_trials} trials)...")
        study_cat = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))
        study_cat.optimize(
            create_catboost_objective(X_train, y_train, cat_cols),
            n_trials=n_trials,
            show_progress_bar=False,
        )
        self.best_params['catboost'] = study_cat.best_params
        
        # ìµœì¢… ëª¨ë¸ í•™ìŠµ
        if verbose:
            print(f"    â””â”€ ìµœì¢… ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        # LightGBM
        lgbm_params = self.best_params['lgbm'].copy()
        lgbm_params.update({'random_state': 42, 'n_jobs': -1, 'verbosity': -1})
        self.models['lgbm'] = lgb.LGBMRegressor(**lgbm_params)
        self.models['lgbm'].fit(X_train, y_train, categorical_feature=cat_cols)
        
        # XGBoost
        xgb_params = self.best_params['xgb'].copy()
        xgb_params.update({'random_state': 42, 'n_jobs': -1, 'verbosity': 0})
        self.models['xgb'] = xgb.XGBRegressor(**xgb_params, enable_categorical=False)
        self.models['xgb'].fit(X_train_encoded, y_train)
        
        # CatBoost
        cat_params = self.best_params['catboost'].copy()
        cat_params.update({'random_seed': 42, 'verbose': False})
        self.models['catboost'] = CatBoostRegressor(**cat_params)
        self.models['catboost'].fit(X_train, y_train, cat_features=cat_cols)
        
        # CVë¡œ ê°€ì¤‘ì¹˜ ê²°ì •
        self._compute_weights(X_train, y_train, cat_cols)
        
        if verbose:
            print(f"    â””â”€ ì•™ìƒë¸” ê°€ì¤‘ì¹˜: LGBM={self.weights['lgbm']:.2f}, "
                  f"XGB={self.weights['xgb']:.2f}, CatBoost={self.weights['catboost']:.2f}")
    
    def _compute_weights(self, X_train, y_train, cat_cols):
        """CV ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚°"""
        X_encoded = X_train.copy()
        for c in cat_cols:
            if c in X_encoded.columns:
                X_encoded[c] = X_encoded[c].astype('category').cat.codes
        
        tscv = TimeSeriesSplit(n_splits=3)
        scores = {'lgbm': [], 'xgb': [], 'catboost': []}
        
        for train_idx, val_idx in tscv.split(X_train):
            X_val = X_train.iloc[val_idx]
            X_val_enc = X_encoded.iloc[val_idx]
            y_val = y_train[val_idx]
            
            preds_lgbm = self.models['lgbm'].predict(X_val)
            preds_xgb = self.models['xgb'].predict(X_val_enc)
            preds_cat = self.models['catboost'].predict(X_val)
            
            scores['lgbm'].append(1 / (smape(y_val, preds_lgbm) + 1e-9))
            scores['xgb'].append(1 / (smape(y_val, preds_xgb) + 1e-9))
            scores['catboost'].append(1 / (smape(y_val, preds_cat) + 1e-9))
        
        # ì—­ SMAPE ê¸°ë°˜ ê°€ì¤‘ì¹˜
        total = sum(np.mean(scores[k]) for k in scores)
        self.weights = {k: np.mean(scores[k]) / total for k in scores}
    
    def predict(self, X: pd.DataFrame) -> np.ndarray:
        """ì•™ìƒë¸” ì˜ˆì¸¡"""
        X_encoded = X.copy()
        for c in self.cat_cols:
            if c in X_encoded.columns:
                X_encoded[c] = X_encoded[c].astype('category').cat.codes
        
        preds_lgbm = self.models['lgbm'].predict(X)
        preds_xgb = self.models['xgb'].predict(X_encoded)
        preds_cat = self.models['catboost'].predict(X)
        
        return (
            self.weights['lgbm'] * preds_lgbm +
            self.weights['xgb'] * preds_xgb +
            self.weights['catboost'] * preds_cat
        )


# ============================================================================
# ë°ì´í„° ì¤€ë¹„
# ============================================================================

def prepare_data(raw_csv: str, encoding: str, external_dir: str, cfg: RadarConfig):
    """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (run_mvp.pyì™€ ë™ì¼í•œ ë°©ì‹)"""
    print("[1/5] ë°ì´í„° ë¡œë“œ...")
    raw = load_internal_csv(raw_csv, encoding=encoding)
    
    print("[2/5] ì„¸ê·¸ë¨¼íŠ¸-ì›”ë³„ ì§‘ê³„...")
    segment_monthly = aggregate_to_segment_month(raw, cfg)
    
    print("[3/5] íŒ¨ë„ êµ¬ì¶•...")
    panel_result = build_full_panel(segment_monthly, cfg)
    panel = panel_result.panel
    
    print("[4/5] KPI ê³„ì‚°...")
    panel = compute_kpis(panel, cfg)
    
    # ì™¸ë¶€ ë°ì´í„° ë¡œë“œ (ìˆìœ¼ë©´)
    external_data = None
    if external_dir and os.path.isdir(external_dir):
        print("[4.5/5] ì™¸ë¶€ ê±°ì‹œê²½ì œ ë°ì´í„° ë³‘í•©...")
        try:
            external_data = load_all_external_data(
                external_dir, 
                file_config=cfg.external_data_files
            )
            if external_data is not None and not external_data.empty:
                panel = merge_external_to_panel(panel, external_data, month_col="month")
                print(f"  ì™¸ë¶€ ë°ì´í„° ë³‘í•© ì™„ë£Œ: íŒ¨ë„ ì»¬ëŸ¼ ìˆ˜ {len(panel.columns)}")
        except Exception as e:
            print(f"  ì™¸ë¶€ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
    
    # íƒ€ê²Ÿ ì»¬ëŸ¼
    model_targets = {
        "ì˜ˆê¸ˆì´ì”ì•¡": "log1p_ì˜ˆê¸ˆì´ì”ì•¡",
        "ëŒ€ì¶œì´ì”ì•¡": "log1p_ëŒ€ì¶œì´ì”ì•¡",
        "ì¹´ë“œì´ì‚¬ìš©": "log1p_ì¹´ë“œì´ì‚¬ìš©",
        "ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡": "log1p_ë””ì§€í„¸ê±°ë˜ê¸ˆì•¡",
        "ìˆœìœ ì…": "slog1p_ìˆœìœ ì…",
    }
    if "log1p_FXì´ì•¡" in panel.columns:
        model_targets["FXì´ì•¡"] = "log1p_FXì´ì•¡"
    
    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    print("[5/5] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§...")
    value_cols = list(model_targets.values()) + ["í•œë„ì†Œì§„ìœ¨", "ë””ì§€í„¸ë¹„ì¤‘", "ìë™ì´ì²´ë¹„ì¤‘", "customer_count"]
    value_cols = [c for c in value_cols if c in panel.columns]
    
    panel_fe = panel.copy()
    panel_fe = add_lag_features(panel_fe, group_col="segment_id", time_col="month", value_cols=value_cols)
    panel_fe = add_rolling_features(panel_fe, group_col="segment_id", time_col="month", value_cols=value_cols)
    panel_fe = add_change_features(panel_fe, group_col="segment_id", time_col="month", value_cols=value_cols)
    
    # ê±°ì‹œê²½ì œ Lag í”¼ì²˜ ì¶”ê°€
    if external_data is not None and not external_data.empty:
        print("  ê±°ì‹œê²½ì œ ë³€ìˆ˜ Lag í”¼ì²˜ ìƒì„± ì¤‘...")
        macro_cols_available = [c for c in cfg.macro_feature_cols if c in panel_fe.columns]
        if macro_cols_available:
            panel_fe = add_macro_lag_features(
                panel_fe, 
                macro_cols=macro_cols_available, 
                lags=cfg.macro_lag_periods
            )
            print(f"  ê±°ì‹œê²½ì œ Lag í”¼ì²˜ ì¶”ê°€ ì™„ë£Œ: {len(macro_cols_available)}ê°œ ë³€ìˆ˜ x {len(cfg.macro_lag_periods)}ê°œ Lag")
    
    return panel_fe


def prepare_training_data(
    panel: pd.DataFrame,
    kpi_col: str,
    horizon: int,
    train_end: pd.Timestamp,
    cfg: RadarConfig,
    is_net_flow: bool = False,
):
    """í•™ìŠµ ë°ì´í„° ì¤€ë¹„"""
    data = panel.sort_values(['segment_id', 'month']).copy()
    
    # ìˆœìœ ì…(net flow)ì˜ ê²½ìš° ì¶”ê°€ í”¼ì²˜ ìƒì„±
    if is_net_flow:
        print("    â””â”€ ìˆœìœ ì… íŠ¹ë³„ í”¼ì²˜ ì¶”ê°€ ì¤‘...")
        data = add_net_flow_features(data)
    
    # íƒ€ê²Ÿ ìƒì„±
    target_col = f"target__h{horizon}"
    data[target_col] = data.groupby('segment_id')[kpi_col].shift(-horizon)
    
    # í•„í„°ë§
    data = data[(data.get('pre_birth', 0) == 0)].copy()
    data = data.dropna(subset=[target_col])
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
    train_mask = data['month'] <= train_end
    
    # í”¼ì²˜ ì„ íƒ
    exclude = {kpi_col, target_col, 'month', 'first_month', 'segment_id'}
    feature_cols = [c for c in data.columns if c not in exclude]
    
    categorical_cols = cfg.segment_keys
    feature_cols = [c for c in feature_cols 
                    if (c in categorical_cols) or (pd.api.types.is_numeric_dtype(data[c]))]
    
    cat_cols = [c for c in categorical_cols if c in feature_cols]
    
    X_train = data.loc[train_mask, feature_cols].copy()
    y_train = data.loc[train_mask, target_col].values
    
    X_test = data.loc[~train_mask, feature_cols].copy()
    y_test = data.loc[~train_mask, target_col].values
    
    # ë²”ì£¼í˜• íƒ€ì… ë³€í™˜
    for c in cat_cols:
        if c in X_train.columns:
            X_train[c] = X_train[c].astype('category')
            X_test[c] = X_test[c].astype('category')
    
    return X_train, y_train, X_test, y_test, cat_cols


def add_net_flow_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    ìˆœìœ ì… ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¹ë³„ í”¼ì²˜ ì¶”ê°€
    ìˆœìœ ì… = ìš”êµ¬ë¶ˆì…ê¸ˆê¸ˆì•¡ - ìš”êµ¬ë¶ˆì¶œê¸ˆê¸ˆì•¡
    """
    out = df.copy()
    
    # 1. ì…ê¸ˆ/ì¶œê¸ˆ ê°œë³„ Lag í”¼ì²˜
    flow_cols = ['ìš”êµ¬ë¶ˆì…ê¸ˆê¸ˆì•¡', 'ìš”êµ¬ë¶ˆì¶œê¸ˆê¸ˆì•¡']
    for col in flow_cols:
        if col in out.columns:
            for lag in [1, 2, 3]:
                out[f'{col}__lag{lag}'] = out.groupby('segment_id')[col].shift(lag)
    
    # 2. ì…ê¸ˆ/ì¶œê¸ˆ ë¹„ìœ¨
    if 'ìš”êµ¬ë¶ˆì…ê¸ˆê¸ˆì•¡' in out.columns and 'ìš”êµ¬ë¶ˆì¶œê¸ˆê¸ˆì•¡' in out.columns:
        eps = 1e-9
        out['ì…ì¶œê¸ˆë¹„ìœ¨'] = out['ìš”êµ¬ë¶ˆì…ê¸ˆê¸ˆì•¡'] / (out['ìš”êµ¬ë¶ˆì¶œê¸ˆê¸ˆì•¡'] + eps)
        out['ì…ì¶œê¸ˆë¹„ìœ¨'] = out['ì…ì¶œê¸ˆë¹„ìœ¨'].clip(0, 10)  # ì´ìƒì¹˜ ì œí•œ
        
        # ë¹„ìœ¨ì˜ Lag
        for lag in [1, 2, 3]:
            out[f'ì…ì¶œê¸ˆë¹„ìœ¨__lag{lag}'] = out.groupby('segment_id')['ì…ì¶œê¸ˆë¹„ìœ¨'].shift(lag)
    
    # 3. ìˆœìœ ì…ì˜ ë¶€í˜¸ ë³€í™” (ë°©í–¥ ì „í™˜ ê°ì§€)
    if 'slog1p_ìˆœìœ ì…' in out.columns:
        out['ìˆœìœ ì…ë¶€í˜¸'] = np.sign(out['slog1p_ìˆœìœ ì…'])
        out['ìˆœìœ ì…ë¶€í˜¸__lag1'] = out.groupby('segment_id')['ìˆœìœ ì…ë¶€í˜¸'].shift(1)
        out['ìˆœìœ ì…ë¶€í˜¸ë³€í™”'] = (out['ìˆœìœ ì…ë¶€í˜¸'] != out['ìˆœìœ ì…ë¶€í˜¸__lag1']).astype(int)
    
    # 4. ì˜ˆê¸ˆ ì”ì•¡ ëŒ€ë¹„ ìˆœìœ ì… ë¹„ìœ¨
    if 'ìˆœìœ ì…' in out.columns and 'log1p_ì˜ˆê¸ˆì´ì”ì•¡' in out.columns:
        out['ìˆœìœ ì…_ì˜ˆê¸ˆë¹„ìœ¨'] = out['ìˆœìœ ì…'] / (np.expm1(out['log1p_ì˜ˆê¸ˆì´ì”ì•¡']) + 1e-9)
        out['ìˆœìœ ì…_ì˜ˆê¸ˆë¹„ìœ¨'] = out['ìˆœìœ ì…_ì˜ˆê¸ˆë¹„ìœ¨'].clip(-1, 1)
    
    # 5. Rolling í†µê³„ (ìˆœìœ ì… ë³€ë™ì„±)
    if 'slog1p_ìˆœìœ ì…' in out.columns:
        for w in [3, 6]:
            shifted = out.groupby('segment_id')['slog1p_ìˆœìœ ì…'].shift(1)
            out[f'ìˆœìœ ì…__roll{w}_std'] = shifted.rolling(w, min_periods=1).std()
            out[f'ìˆœìœ ì…__roll{w}_mean'] = shifted.rolling(w, min_periods=1).mean()
    
    return out


# ============================================================================
# ë©”ì¸
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="ëª¨ë¸ ê°œì„  v2")
    parser.add_argument("--raw_csv", required=True, help="ì›ë³¸ ë°ì´í„° ê²½ë¡œ")
    parser.add_argument("--encoding", default="utf-8-sig", help="ì¸ì½”ë”©")
    parser.add_argument("--external_dir", default="../ì™¸ë¶€ ë°ì´í„°", help="ì™¸ë¶€ ë°ì´í„° ë””ë ‰í† ë¦¬")
    parser.add_argument("--train_end", default="2024-09", help="í•™ìŠµ ì¢…ë£Œì›” (YYYY-MM)")
    parser.add_argument("--n_trials", type=int, default=20, help="Optuna ì‹œë„ íšŸìˆ˜")
    parser.add_argument("--out_dir", default="outputs/improved_models", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    args = parser.parse_args()
    
    cfg = RadarConfig()
    os.makedirs(args.out_dir, exist_ok=True)
    
    train_end = pd.Timestamp(args.train_end)
    
    # ë°ì´í„° ì¤€ë¹„
    panel = prepare_data(args.raw_csv, args.encoding, args.external_dir, cfg)
    
    # KPI ëª©ë¡
    kpi_cols = list(cfg.kpi_defs.keys())
    
    # ê²°ê³¼ ì €ì¥
    results = []
    
    print("\n" + "=" * 60)
    print("ğŸš€ ëª¨ë¸ ê°œì„  ì‹œì‘")
    print("=" * 60)
    
    for kpi in kpi_cols:
        # ë¡œê·¸ ë³€í™˜ëœ KPI ì°¾ê¸°
        if kpi == "ìˆœìœ ì…":
            log_kpi = f"slog1p_{kpi}"
        else:
            log_kpi = f"log1p_{kpi}"
        
        if log_kpi not in panel.columns:
            print(f"âš ï¸ {log_kpi} ì»¬ëŸ¼ ì—†ìŒ, ê±´ë„ˆëœ€")
            continue
        
        for horizon in cfg.horizons:
            print(f"\n[{kpi}] Horizon {horizon}ê°œì›”")
            print("-" * 40)
            
            # ë°ì´í„° ì¤€ë¹„ (ìˆœìœ ì…ì€ íŠ¹ë³„ ì²˜ë¦¬)
            is_net_flow = (kpi == "ìˆœìœ ì…")
            X_train, y_train, X_test, y_test, cat_cols = prepare_training_data(
                panel, log_kpi, horizon, train_end, cfg, is_net_flow=is_net_flow
            )
            
            if len(X_train) < 100 or len(X_test) < 10:
                print(f"  âš ï¸ ë°ì´í„° ë¶€ì¡±: train={len(X_train)}, test={len(X_test)}")
                continue
            
            print(f"  ğŸ“Š Train: {len(X_train):,}, Test: {len(X_test):,}, Features: {len(X_train.columns)}")
            
            # ìˆœìœ ì…ì€ ë” ë§ì€ íŠœë‹ ì‹œë„
            n_trials = args.n_trials * 2 if kpi == "ìˆœìœ ì…" else args.n_trials
            
            # ì•™ìƒë¸” í•™ìŠµ
            ensemble = EnsembleForecaster(kpi=kpi, horizon=horizon)
            ensemble.tune_and_train(X_train, y_train, cat_cols, n_trials=n_trials)
            
            # í…ŒìŠ¤íŠ¸ í‰ê°€
            preds = ensemble.predict(X_test)
            
            metrics = {
                'kpi': kpi,
                'horizon': horizon,
                'n_train': len(X_train),
                'n_test': len(X_test),
                'mae': mean_absolute_error(y_test, preds),
                'rmse': rmse(y_test, preds),
                'smape': smape(y_test, preds),
                'wmape': wmape(y_test, preds),
                'r2': r2_score(y_test, preds),
                'lgbm_weight': ensemble.weights['lgbm'],
                'xgb_weight': ensemble.weights['xgb'],
                'catboost_weight': ensemble.weights['catboost'],
            }
            
            results.append(metrics)
            
            print(f"  âœ… SMAPE: {metrics['smape']:.2f}%, RÂ²: {metrics['r2']:.4f}")
    
    # ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame(results)
    results_path = os.path.join(args.out_dir, "improved_metrics.csv")
    results_df.to_csv(results_path, index=False)
    print(f"\nğŸ“ ê²°ê³¼ ì €ì¥: {results_path}")
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 60)
    print("ğŸ“Š ê°œì„  ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print(results_df[['kpi', 'horizon', 'smape', 'wmape', 'r2']].to_string(index=False))
    
    print("\nâœ… ëª¨ë¸ ê°œì„  ì™„ë£Œ!")


if __name__ == "__main__":
    main()
